{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["amP3pcx1chpX","eBNOEo6GgYOF","q5Khe59cl-VD","XUaN0CJBqL_K","U75-Zq4DsSBR","bk1YZ6B4xdsX"],"gpuType":"T4","authorship_tag":"ABX9TyP3pg6dnyYKTbfPhtt1zGQN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Stock Price Prediction Models\n","\n","This notebook features example code for every model used in the stock price prediction project, including statistical and AI-based approaches.\n","\n","**Important Notes for Running this Notebook:**\n","\n","*   **Environment:** This notebook is designed to run in Google Colab, which is a free cloud-based environment. Running locally may not work without significant setup.\n","*   **Hardware Acceleration:** I strongly recommend connecting to the **T4 GPU runtime** immediately (Runtime -> Change runtime type -> T4 GPU). The AI models later in the notebook are computationally intensive and will not run in a reasonable time on a CPU. Changing the runtime after loading data will require you to re-run the data acquisition section.\n","*   **Execution Time:** Running all cells in this notebook takes approximately 90 minutes on the T4 GPU.\n","*   **Data Dependency:** Please ensure you run the first section (\"Acquiring data\") completely, as it provides the necessary data for all subsequent sections and models."],"metadata":{"id":"0C-UAg2jcx4j"}},{"cell_type":"markdown","source":["# 1. Acquiring data"],"metadata":{"id":"amP3pcx1chpX"}},{"cell_type":"markdown","source":["Define the list of tickers for which OHLCV data will be downloaded."],"metadata":{"id":"w_JPBGB28gpi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOAB2J3DcCH1"},"outputs":[],"source":["import pandas as pd\n","\n","data = pd.read_csv(\"http://www.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt\", sep='|')\n","# This website contains information about every security listed on the NASDAQ.\n","data_clean = data[data['Test Issue'] == 'N']\n","\n","# Load 1 of the 3 options by uncommenting a line. Third option provides sufficient data for all of this notebook.\n","\n","# symbols = data_clean['NASDAQ Symbol'].tolist() # Complete NASDAQ set. Not needed for this notebook.\n","# symbols = ['AAPL', 'JPM', 'NVDA', 'SPY'] # The 4 securities sufficient for predictions apart from GNNs and TFTGNN.\n","symbols = ['AAPL', 'JPM', 'NVDA', 'GOOGL', 'MSFT', 'AMZN', 'META',\n","           'TSLA', 'SONY', 'JBL', 'NFLX',  'AMD', 'TSM', 'ORCL', 'AVGO',\n","           'V', 'MS', 'COF', 'MET', 'AMAT', 'MCHP', 'ADI',\n","           'BAC', 'BLK', \"ASML\", 'QCOM', 'CRM', 'ADBE', 'GS', 'WFC',\n","           'SCHW', 'BK', 'AXP', 'TXN', 'MU', 'NXPI', 'KLAC', 'LRCX',\n","           'UNH', 'JNJ', 'PFE', 'MRK', 'LLY', 'TMO', 'BMY', 'LMT', 'F',\n","           'GM', 'HMC', 'TM', 'WMT', 'HD', 'COST', 'PG', 'KO', 'MCD',\n","           'TGT', 'PEP', 'JNJ', 'NVO', 'SPY', 'QQQ', 'DIA', 'IWM', 'XLK', 'XLF',\n","           'XLE', 'XLI', 'XLV', 'XLY', 'XLP', 'VNQ', 'IYR', 'VGT', 'VTI', 'VUG',\n","           'VTV', 'IWF', 'IWD', 'ITOT'] # These 80 securities are necessary to run the GNN and TFT-GNN models.\n","\n","print('total number of symbols traded = {}'.format(len(symbols)))"]},{"cell_type":"markdown","source":["Install yfinance:"],"metadata":{"id":"lXRX9RlifFMb"}},{"cell_type":"code","source":["! pip install yfinance > /dev/null 2>&1"],"metadata":{"id":"2uqTnUXOcvwF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use yfinance to acquire the data for each stock in the symbols dataframe. Downloading the recommended 80 tickers should work in one go. To download data for the full set of NASDAQ securities, you will hit Yahoo Finance API limits and have to do it in chunks."],"metadata":{"id":"vrV6X5QZfJvI"}},{"cell_type":"code","source":["import yfinance as yf\n","import os, contextlib\n","\n","offset = 0\n","limit = 1000\n","period = \"max\" # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n","\n","limit = limit if limit else len(symbols)\n","end = min(offset + limit, len(symbols))\n","is_valid = [False] * len(symbols)\n","# force silencing of verbose API\n","with open(os.devnull, 'w') as devnull:\n","    with contextlib.redirect_stdout(devnull):\n","        for i in range(offset, end):\n","            s = symbols[i]\n","            data = yf.download(s, period=period)\n","            if len(data.index) == 0:\n","                continue\n","\n","            is_valid[i] = True\n","            data.to_csv('{}.csv'.format(s))\n","\n","print('Total number of valid symbols downloaded = {}'.format(sum(is_valid)))"],"metadata":{"id":"uzXuaBOLfCOb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Statistical Models"],"metadata":{"id":"eBNOEo6GgYOF"}},{"cell_type":"markdown","source":["## 2.1 SARIMA"],"metadata":{"id":"-QIjUJkjgeEY"}},{"cell_type":"markdown","source":["Full year blind prediction, currently configured for AAPL 2018."],"metadata":{"id":"XIuHRZV4httM"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n","from itertools import product\n","from joblib import Parallel, delayed\n","import warnings\n","warnings.filterwarnings('ignore')\n","import gc\n","\n","# Read CSV skipping the first two metadata rows\n","df = pd.read_csv('AAPL.csv', skiprows=3, header=None) # You can change AAPL to any of the loaded tickers.\n","\n","# Set proper column names\n","df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n","df.set_index('Date', inplace=True)\n","\n","train = df.loc['2015-01-01':'2017-12-31']['Close']\n","test = df.loc['2018-01-01':'2018-12-31']['Close']\n","\n","# Model and Hyperparameters\n","model = SARIMAX(train, order=(2,1,3), seasonal_order=(1,1,1,22), enforce_stationarity=False, enforce_invertibility=False)\n","results = model.fit(disp=False)\n","forecast_result = results.get_forecast(steps=len(test))\n","forecast = forecast_result.predicted_mean\n","forecast_ci = forecast_result.conf_int() # confidence interval\n","\n","# Set the index of forecast_ci to match the index of the test data\n","forecast_ci.index = test.index\n","\n","plt.figure(figsize=(10,4))\n","plt.plot(train.index, train, label='Train')\n","plt.plot(test.index, test, label='Test')\n","plt.plot(test.index, forecast, label='Forecast')\n","plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='pink')\n","plt.legend()\n","plt.title('SARIMA Forecast')\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Close Price\")\n","plt.show()\n","\n","# Metrics\n","rmse = np.sqrt(mean_squared_error(test, forecast))\n","mae = mean_absolute_error(test, forecast)\n","mape = mean_absolute_percentage_error(test, forecast)\n","r2 = r2_score(test, forecast)\n","print(f'RMSE: {rmse:.2f}, MAE: {mae:.2f}', f'MAPE: {mape:.2f}', f'R2: {r2:.2f}')"],"metadata":{"id":"w9iA8DHNfgXQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Weekly sliding-window predictions. Note that this snippet takes a 5-15 minutes to run. Currently configured for AAPL 2021."],"metadata":{"id":"Bbhp_Yo0i0qt"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","from joblib import Parallel, delayed\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# -------------------------------\n","# Load and preprocess data\n","# -------------------------------\n","df = pd.read_csv('AAPL.csv', skiprows=3, header=None)\n","df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","df['Date'] = pd.to_datetime(df['Date'])\n","df.set_index('Date', inplace=True)\n","\n","series = df['Close']\n","\n","# -------------------------------\n","# Hyperparameters\n","# -------------------------------\n","best_order = (2, 1, 3)\n","best_seasonal_order = (0, 1, 1, 22)\n","\n","# -------------------------------\n","# Forecast settings\n","# -------------------------------\n","forecast_horizon = 5\n","step_size = 5\n","train_window_days = 500\n","\n","forecast_dates = pd.date_range(\"2021-01-01\", \"2021-12-31\", freq=f\"{step_size}D\")\n","\n","# -------------------------------\n","# Forecasting function\n","# -------------------------------\n","def forecast_next_5_days(forecast_start):\n","    train_end = forecast_start - pd.Timedelta(days=1)\n","    train_start = train_end - pd.Timedelta(days=train_window_days - 1)\n","\n","    if train_start < series.index[0]:\n","        return []\n","\n","    train_data = series[train_start:train_end]\n","\n","    try:\n","        model = SARIMAX(train_data,\n","                        order=best_order,\n","                        seasonal_order=best_seasonal_order,\n","                        enforce_stationarity=False,\n","                        enforce_invertibility=False)\n","        results = model.fit(disp=False, method_kwargs={\"maxiter\": 50})\n","        forecast = results.get_forecast(steps=forecast_horizon)\n","        preds = forecast.predicted_mean\n","    except:\n","        return []\n","\n","    results_list = []\n","    for offset, pred_date in enumerate(pd.date_range(forecast_start, periods=forecast_horizon)):\n","        if pred_date not in series.index:\n","            continue\n","        results_list.append({\n","            \"Date\": pred_date,\n","            \"Actual\": series[pred_date],\n","            \"Predicted\": preds.iloc[offset]\n","        })\n","    return results_list\n","\n","# -------------------------------\n","# Run forecasts in parallel\n","# -------------------------------\n","all_results = Parallel(n_jobs=-1)(\n","    delayed(forecast_next_5_days)(day) for day in tqdm(forecast_dates)\n",")\n","\n","# Flatten list of lists\n","forecast_flat = [item for sublist in all_results for item in sublist]\n","\n","# -------------------------------\n","# Evaluation & Plotting\n","# -------------------------------\n","forecast_df = pd.DataFrame(forecast_flat).dropna()\n","forecast_df.set_index(\"Date\", inplace=True)\n","\n","rmse = np.sqrt(mean_squared_error(forecast_df[\"Actual\"], forecast_df[\"Predicted\"]))\n","mae = mean_absolute_error(forecast_df[\"Actual\"], forecast_df[\"Predicted\"])\n","mape = mean_absolute_percentage_error(forecast_df[\"Actual\"], forecast_df[\"Predicted\"])\n","r2 = r2_score(forecast_df[\"Actual\"], forecast_df[\"Predicted\"])\n","\n","print(\"\\nSliding 5-Day Forecast Metrics:\")\n","print(f\"  RMSE: {rmse:.4f}\")\n","print(f\"  MAE:  {mae:.4f}\")\n","print(f\"  MAPE: {mape:.4f}\")\n","print(f\"  R2:   {r2:.4f}\")\n","\n","# Plot\n","plt.figure(figsize=(12, 6))\n","plt.plot(series[\"2021\"], label=\"Actual\")\n","plt.plot(forecast_df.index, forecast_df[\"Predicted\"], label=\"5-day Forecasts\", alpha=0.7)\n","plt.title(\"SARIMA: 5-Day-Ahead Forecasts Every 5 Days (2021)\")\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.tight_layout()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"FDj0K8jAjF6A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 ETS"],"metadata":{"id":"KwvN9WgUjvXv"}},{"cell_type":"markdown","source":["The code largely follows the same structure as SARIMA's. Below is the daily rolling-window prediction for AAPL 2024. This takes 1-2 minutes to run."],"metadata":{"id":"Z90FbBv5kDA9"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from itertools import product\n","from statsmodels.tsa.holtwinters import ExponentialSmoothing\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n","from joblib import Parallel, delayed\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# -------------------------------\n","# Load and preprocess data\n","# -------------------------------\n","df = pd.read_csv('AAPL.csv', skiprows=3, header=None)\n","df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","df['Date'] = pd.to_datetime(df['Date'])\n","df.set_index('Date', inplace=True)\n","series = df['Close']\n","\n","# -------------------------------\n","# Forecast settings\n","# -------------------------------\n","forecast_horizon = 1\n","step_size = 1\n","train_window_days = 750\n","\n","forecast_dates = series.loc[\"2024-01-01\":\"2024-12-31\"].index[::step_size]\n","\n","# -------------------------------\n","# Forecasting function\n","# -------------------------------\n","def forecast_next_5_days(forecast_start):\n","    train_end = forecast_start - pd.Timedelta(days=1)\n","    train_start = train_end - pd.Timedelta(days=train_window_days - 1)\n","\n","    if train_start < series.index[0]:\n","        return []\n","\n","    train_data = series[train_start:train_end]\n","\n","    try:\n","        model = ExponentialSmoothing(train_data, trend='add', seasonal='mul',\n","                                     seasonal_periods=5, damped_trend=False)\n","        results = model.fit()\n","        forecast = results.forecast(steps=forecast_horizon)\n","    except Exception as e:\n","        print(f\"Error forecasting for date {forecast_start}: {e}\")\n","        return []\n","\n","    results_list = []\n","    forecast_dates_range = pd.date_range(forecast_start, periods=forecast_horizon)\n","    for offset, pred_date in enumerate(forecast_dates_range):\n","        if pred_date in series.index and offset < len(forecast):\n","             results_list.append({\n","                \"Date\": pred_date,\n","                \"Actual\": series[pred_date],\n","                \"Predicted\": forecast.iloc[offset]\n","            })\n","    return results_list\n","\n","\n","# -------------------------------\n","# Run forecasts in parallel\n","# -------------------------------\n","all_results = Parallel(n_jobs=-1)(\n","    delayed(forecast_next_5_days)(day) for day in tqdm(forecast_dates)\n",")\n","\n","# Flatten list of lists\n","forecast_flat = [item for sublist in all_results for item in sublist]\n","\n","# -------------------------------\n","# Evaluation & Plotting\n","# -------------------------------\n","forecast_df = pd.DataFrame(forecast_flat).dropna()\n","forecast_df.set_index(\"Date\", inplace=True)\n","\n","rmse = np.sqrt(mean_squared_error(forecast_df[\"Actual\"], forecast_df[\"Predicted\"]))\n","mae = mean_absolute_error(forecast_df[\"Actual\"], forecast_df[\"Predicted\"])\n","mape = mean_absolute_percentage_error(forecast_df[\"Actual\"], forecast_df[\"Predicted\"])\n","r2 = r2_score(forecast_df[\"Actual\"], forecast_df[\"Predicted\"])\n","\n","print(\"\\nSliding 5-Day Forecast Metrics:\")\n","print(f\"  RMSE: {rmse:.4f}\")\n","print(f\"  MAE:  {mae:.4f}\")\n","print(f\"  MAPE: {mape:.4f}\")\n","print(f\"  R2:   {r2:.4f}\")\n","\n","# Plot\n","plt.figure(figsize=(12, 6))\n","plt.plot(series[\"2024\"], label=\"Actual\")\n","plt.plot(forecast_df.index, forecast_df[\"Predicted\"], label=\"5-day Forecasts\", alpha=0.7)\n","plt.title(\"ETS: Daily rolling-window Predictions, AAPL 2024\")\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"pzaWwM_CjTsr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 BSTS"],"metadata":{"id":"3NTShEfCkZNd"}},{"cell_type":"markdown","source":["This project only did full year predictions for BSTS. Unfortunately, the orbit package, necessary for this model, takes 20 minutes to build itself. The code is included nevertheless. Currently configured to predict JPM 2018."],"metadata":{"id":"RVOHAT4ukh5x"}},{"cell_type":"code","source":["!pip install orbit-ml"],"metadata":{"id":"num-Bxa2lJ4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from orbit.models import DLT\n","from orbit.diagnostics.metrics import smape, mape\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gc\n","\n","# -------------------------------\n","# Load and preprocess data\n","# -------------------------------\n","df = pd.read_csv('JPM.csv', skiprows=3, header=None)\n","df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n","df = df[['Date', 'Close']]\n","df.columns = ['ds', 'y']\n","df = df.set_index('ds').asfreq('D')\n","df['y'] = df['y'].interpolate(method='time')\n","df = df.reset_index()\n","\n","train_start = '2015-01-01'\n","train_end = '2017-12-31'\n","test_start = '2018-01-01'\n","test_end = '2018-12-31'\n","\n","seasonality = 22\n","\n","train_df = df[(df['ds'] >= train_start) & (df['ds'] <= train_end)].copy()\n","test_df = df[(df['ds'] >= test_start) & (df['ds'] <= test_end)].copy()\n","\n","model = DLT(\n","    response_col='y',\n","    date_col='ds',\n","    seasonality=seasonality,\n","    seed=2023, # Reproducibility\n","    estimator='stan-mcmc',\n","    )\n","model.fit(train_df)\n","\n","    # Forecast\n","forecast_df = model.predict(df=test_df)\n","y_true = test_df['y'].values\n","y_pred = forecast_df['prediction'].values\n","\n","    # Evaluate\n","rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n","mae_val = mean_absolute_error(y_true, y_pred)\n","mape_val = mape(y_true, y_pred)\n","smape_val = smape(y_true, y_pred)\n","r2_val = r2_score(y_true, y_pred)\n","\n","print(f\"Metrics:\")\n","print(f\"  RMSE:  {rmse_val:.4f}\")\n","print(f\"  MAE:   {mae_val:.4f}\")\n","print(f\"  MAPE:  {mape_val:.4f}\")\n","print(f\"  SMAPE: {smape_val:.4f}\")\n","print(f\"  R2:    {r2_val:.4f}\")\n","\n","# Plot\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_df['ds'], train_df['y'], label='Train')\n","plt.plot(test_df['ds'], test_df['y'], label='Test')\n","plt.plot(forecast_df['ds'], forecast_df['prediction'], label='Forecast')\n","plt.fill_between(forecast_df['ds'], forecast_df['prediction_5'], forecast_df['prediction_95'], color='pink', alpha=0.3)\n","plt.title(f'Seasonality {seasonality} | BSTS Forecast (DLT model)')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"6zviswnplPrK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. AI Models"],"metadata":{"id":"q5Khe59cl-VD"}},{"cell_type":"markdown","source":["## 3.1 LSTM"],"metadata":{"id":"cNxKqghnmEV7"}},{"cell_type":"markdown","source":["The ta package is used in the LSTM code. Installation takes 10 seconds."],"metadata":{"id":"0dkrJTs7oL5i"}},{"cell_type":"code","source":["!pip install ta"],"metadata":{"id":"BSSGIRPKoGB2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code predicts all 4 of the selected stocks (AAPL, JPM, NVDA, SPY) using a daily rolling-window. Currently configured to do this in 2024. This will run in 10-20 minutes on Colab's default CPU, or 5-6 minutes on the T4 GPU."],"metadata":{"id":"gRnTCSTFo2I_"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","import warnings\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","from keras.optimizers import Adam\n","from keras.layers import Dropout\n","from keras.callbacks import EarlyStopping\n","from dateutil.relativedelta import relativedelta\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","def load_main_stock(path):\n","    df = pd.read_csv(path, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']].set_index('Date')\n","    df = df.loc['2008-01-01':'2024-12-31']\n","\n","    df['Close_lag1'] = df['Close'].shift(1)\n","    df['Volume_lag1'] = df['Volume'].shift(1)\n","    df['ma_5'] = df['Close_lag1'].rolling(window=5).mean()\n","    df['volatility_63'] = df['Close_lag1'].rolling(window=63).std()\n","    df['intraday_range'] = df['High'].shift(1) - df['Low'].shift(1)\n","    df['upper_shadow'] = df['High'].shift(1) - df[['Open', 'Close']].shift(1).max(axis=1)\n","    df['lower_shadow'] = df[['Open', 'Close']].shift(1).min(axis=1) - df['Low'].shift(1)\n","    df['momentum_126'] = df['Close_lag1'] - df['Close_lag1'].shift(126)\n","    df['RSI'] = RSIIndicator(close=df['Close_lag1'], window=14).rsi()\n","    macd = MACD(close=df['Close_lag1'])\n","    df['MACD'] = macd.macd()\n","    df['MACD_signal'] = macd.macd_signal()\n","\n","\n","    df = df.drop(columns=['Open', 'High', 'Low', 'Volume'])\n","    return df.dropna()\n","\n","def create_sequences(X, y, window):\n","    Xs, ys = [], []\n","    for i in range(len(X) - window):\n","        Xs.append(X[i:i+window])\n","        ys.append(y[i+window])\n","    return np.array(Xs), np.array(ys)\n","\n","def forecast_with_monthly_retraining(df, stock_name, window=20):\n","    df = df[['Close', 'Close_lag1']].dropna()\n","    feature_cols = df.columns.drop('Close')\n","    df = df.loc['2018-01-01':'2024-12-31']\n","\n","    test_start = pd.Timestamp(\"2023-12-11\")\n","    test_end = pd.Timestamp(\"2024-12-31\")\n","\n","    current_train_end = pd.Timestamp(\"2023-12-31\")\n","    current_test_start = test_start\n","\n","    all_preds = []\n","    all_true = []\n","    all_dates = []\n","\n","    while current_test_start <= test_end:\n","        current_test_end = (current_test_start + relativedelta(months=1)) - pd.Timedelta(days=1)\n","        if current_test_end > test_end:\n","            current_test_end = test_end\n","\n","        train_df = df.loc[:current_train_end]\n","        test_df = df.loc[current_test_start - pd.Timedelta(days=window):current_test_end]\n","\n","        if len(test_df) <= window:\n","            break\n","\n","        # Scaling\n","        scaler_X = MinMaxScaler()\n","        scaler_y = MinMaxScaler()\n","\n","        X_train = scaler_X.fit_transform(train_df[feature_cols])\n","        y_train = scaler_y.fit_transform(train_df[['Close']])\n","\n","        X_test = scaler_X.transform(test_df[feature_cols])\n","        y_test = scaler_y.transform(test_df[['Close']])\n","\n","        X_train_seq, y_train_seq = create_sequences(X_train, y_train, window)\n","        X_test_seq, y_test_seq = create_sequences(X_test, y_test, window)\n","\n","        # Model\n","        model = Sequential([\n","            LSTM(64, input_shape=(window, X_train_seq.shape[2])),\n","            Dense(1)\n","        ])\n","        model.compile(optimizer=Adam(learning_rate=0.003), loss='mse')\n","        model.fit(X_train_seq, y_train_seq, epochs=20, batch_size=64, verbose=0)\n","\n","        y_pred_scaled = model.predict(X_test_seq, verbose=0)\n","        y_pred = scaler_y.inverse_transform(y_pred_scaled)\n","        y_true = scaler_y.inverse_transform(y_test_seq)\n","\n","        # Save results\n","        pred_dates = test_df.index[window:]\n","        all_preds.extend(y_pred.flatten())\n","        all_true.extend(y_true.flatten())\n","        all_dates.extend(pred_dates)\n","\n","        # Append true values to training data for next iteration\n","        past_and_current = df.loc[:current_test_end]\n","        current_train_end = current_test_end + pd.Timedelta(days=0)\n","        current_test_start = current_test_end + pd.Timedelta(days=1)\n","\n","    # Convert to pandas Series with datetime index\n","    pred_series = pd.Series(all_preds, index=pd.to_datetime(all_dates))\n","    true_series = pd.Series(all_true, index=pd.to_datetime(all_dates))\n","\n","    # Truncate to only predictions from Jan 1, 2024\n","    plot_start = pd.Timestamp(\"2024-01-01\")\n","    pred_series = pred_series[pred_series.index >= plot_start]\n","    true_series = true_series[true_series.index >= plot_start]\n","\n","    # Use for metrics\n","    rmse = np.sqrt(mean_squared_error(true_series, pred_series))\n","    mae = mean_absolute_error(true_series, pred_series)\n","    r2 = r2_score(true_series, pred_series)\n","    mape = mean_absolute_percentage_error(true_series, pred_series)\n","\n","    print(f\"\\n Results for {stock_name}\")\n","    print(f\"RMSE: {rmse:.4f}\")\n","    print(f\"MAE : {mae:.4f}\")\n","    print(f\"R^2 : {r2:.4f}\")\n","    print(f\"MAPE: {mape:.4f}\")\n","\n","    # Convert to pandas Series for easy filtering\n","    pred_series = pd.Series(all_preds, index=pd.to_datetime(all_dates))\n","    true_series = pd.Series(all_true, index=pd.to_datetime(all_dates))\n","\n","    # Filter to start from Jan 1, 2024\n","    plot_start = pd.Timestamp(\"2024-01-01\")\n","    pred_series = pred_series[pred_series.index >= plot_start]\n","    true_series = true_series[true_series.index >= plot_start]\n","\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(true_series.index, true_series.values, label='Actual Close')\n","    plt.plot(pred_series.index, pred_series.values, label='Predicted Close')\n","    plt.title(f'{stock_name} - Monthly Retraining Forecast')\n","    plt.xlabel('Date')\n","    plt.ylabel('Close Price')\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","    scaled_rmse = rmse / (np.max(all_true) - np.min(all_true))\n","    return scaled_rmse\n","\n","scaled_rmses = []\n","\n","for ticker in ['AAPL.csv', 'JPM.csv', 'NVDA.csv', 'SPY.csv']:\n","    df = load_main_stock(ticker)\n","    scaled_rmse = forecast_with_monthly_retraining(df, ticker.replace('.csv', ''))\n","    scaled_rmses.append(scaled_rmse)\n","\n","mean_scaled_rmse = sum(scaled_rmses) / len(scaled_rmses)\n","print(\" Mean Scaled RMSE over 4 stocks:\", mean_scaled_rmse)\n"],"metadata":{"id":"KhXs_nA6mGkn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 TFT"],"metadata":{"id":"XUaN0CJBqL_K"}},{"cell_type":"markdown","source":["This does not run in a reasonable time on the CPU. Change the runtime to the T4 GPU, if still using default CPU."],"metadata":{"id":"yKeEjkT_qQDg"}},{"cell_type":"code","source":["!pip install pytorch_forecasting\n","!pip install torch\n","!pip install pytorch_lightning\n","!pip install ta"],"metadata":{"id":"dgK_dSfnrJNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Like LSTM, this code also predicts AAPL, JPM, NVDA and SPY together, so they all need to be loaded. Currently configured for 2021. Takes 5-6 minutes on T4 GPU."],"metadata":{"id":"XJR7MmWarYDd"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n","from pytorch_forecasting.metrics import SMAPE\n","from pytorch_lightning import Trainer, LightningModule\n","from pytorch_lightning.callbacks import EarlyStopping\n","from pytorch_lightning.loggers import TensorBoardLogger\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from collections import defaultdict\n","import warnings\n","import logging\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","warnings.filterwarnings(\"ignore\")\n","logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\n","\n","# === Load and preprocess data for all stocks ===\n","def load_stock(file_path, stock_name):\n","    df = pd.read_csv(file_path, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df['group'] = stock_name\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","    df['upper_shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)\n","    df['lower_shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']\n","    return df\n","\n","aapl = load_stock(\"AAPL.csv\", \"AAPL\")\n","jpm  = load_stock(\"JPM.csv\", \"JPM\")\n","nvda = load_stock(\"NVDA.csv\", \"NVDA\")\n","spy  = load_stock(\"SPY.csv\",  \"SPY\")\n","\n","df = pd.concat([jpm, nvda, spy, aapl], ignore_index=True)\n","df = df[(df['Date'] > '2015-01-01') & (df['Date'] <= '2021-12-31')]\n","df = df.sort_values(['group', 'Date'])\n","df['time_idx'] = df.groupby('group').cumcount()\n","\n","# Split train/val and test (based on calendar year)\n","train_val_df = df[df['Date'] < \"2021-01-01\"].copy()\n","test_df = df[df['Date'] >= \"2020-11-16\"].copy()\n","\n","# Recalculate time_idx in test_df based on prior max index\n","last_time_idx = train_val_df.groupby(\"group\")[\"time_idx\"].max().reset_index()\n","test_df = test_df.merge(last_time_idx, on=\"group\", suffixes=(\"\", \"_last\"))\n","test_df[\"time_idx\"] = test_df.groupby(\"group\").cumcount() + test_df[\"time_idx_last\"] + 1\n","test_df.drop(columns=[\"time_idx_last\"], inplace=True)\n","\n","# === Define dataset parameters ===\n","max_encoder_length = 30\n","max_prediction_length = 1\n","training_cutoff = train_val_df[\"time_idx\"].max() - 252  # leave last 252 days for validation\n","\n","# === Training dataset ===\n","tft_dataset = TimeSeriesDataSet(\n","    train_val_df[train_val_df.time_idx <= training_cutoff],\n","    time_idx=\"time_idx\",\n","    target=\"Close\",\n","    group_ids=[\"group\"],\n","    max_encoder_length=max_encoder_length,\n","    max_prediction_length=max_prediction_length,\n","    time_varying_known_reals=[\"time_idx\"],\n","    time_varying_unknown_reals=[\"Close\", \"RSI\", \"MACD\", 'upper_shadow', 'lower_shadow'],\n","    static_categoricals=[\"group\"],\n","    allow_missing_timesteps=True\n",")\n","\n","# === Validation dataset (pre-2024 only) ===\n","val_df = train_val_df[(train_val_df.time_idx > (training_cutoff - max_encoder_length))]\n","validation = TimeSeriesDataSet.from_dataset(tft_dataset, val_df, stop_randomization=True)\n","\n","train_loader = tft_dataset.to_dataloader(train=True, batch_size=64, num_workers=0)\n","val_loader = validation.to_dataloader(train=False, batch_size=64, num_workers=0)\n","\n","# === Define LightningModule wrapper ===\n","class TFTLightningModule(LightningModule):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        out = self.model(x)\n","        y_pred = out[0]\n","        loss = self.model.loss(y_pred, y)\n","        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=x['encoder_target'].shape[0])\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        out = self.model(x)\n","        y_pred = out[0]\n","        loss = self.model.loss(y_pred, y)\n","        self.log(\"val_loss\", loss, prog_bar=True, batch_size=x['encoder_target'].shape[0])\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.model.parameters(), lr=self.model.hparams.learning_rate)\n","\n","# === Train TFT model ===\n","tft = TemporalFusionTransformer.from_dataset(\n","    tft_dataset,\n","    learning_rate=0.01,\n","    hidden_size=64,\n","    attention_head_size=2,\n","    dropout=0.1,\n","    loss=SMAPE(),\n","    log_interval=10,\n","    reduce_on_plateau_patience=4,\n",").to(\"cuda\")\n","\n","module = TFTLightningModule(tft)\n","\n","trainer = Trainer(\n","    max_epochs=20,\n","    gradient_clip_val=0.1,\n","    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")],\n","    limit_train_batches=30,\n","    logger=TensorBoardLogger(\"lightning_logs\")\n",")\n","\n","trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n","\n","\n","# === Rolling prediction on 2024 test data ===\n","preds_by_group = defaultdict(list)\n","actuals_by_group = defaultdict(list)\n","\n","for group in test_df[\"group\"].unique():\n","    df_group = test_df[test_df[\"group\"] == group].copy()\n","    df_group = df_group.reset_index(drop=True)\n","    df_group[\"time_idx\"] = np.arange(len(df_group))  # reindex cleanly\n","\n","    steps = len(df_group[\"time_idx\"].unique()) - max_encoder_length - max_prediction_length\n","\n","    for offset in range(steps):\n","        decoder_end_time = offset + max_encoder_length + max_prediction_length - 1\n","        data_slice = df_group[df_group.time_idx <= decoder_end_time].tail(max_encoder_length + max_prediction_length)\n","\n","        if data_slice['time_idx'].nunique() < (max_encoder_length + max_prediction_length):\n","            continue\n","\n","        try:\n","            predict_dataset = TimeSeriesDataSet.from_dataset(tft_dataset, data_slice, stop_randomization=True)\n","            predict_loader = predict_dataset.to_dataloader(train=False, batch_size=1, num_workers=0)\n","\n","            prediction = tft.predict(predict_loader).cpu().numpy().flatten()[0]\n","            actual = data_slice.iloc[-1][\"Close\"]\n","\n","            preds_by_group[group].append(prediction)\n","            actuals_by_group[group].append(actual)\n","        except Exception as e:\n","            print(f\"Skipping {group} at offset {offset} due to error: {e}\")\n","\n","\n","# === Evaluation & Plotting ===\n","for group in preds_by_group:\n","    preds = preds_by_group[group]\n","    acts = actuals_by_group[group]\n","\n","    if len(preds) == 0 or len(acts) == 0:\n","        print(f\"\\nNot enough data to evaluate {group}. Skipping...\")\n","        continue\n","\n","    rmse = np.sqrt(mean_squared_error(acts, preds))\n","    mae = mean_absolute_error(acts, preds)\n","    r2 = r2_score(acts, preds)\n","    mape = np.mean(np.abs((np.array(acts) - np.array(preds)) / np.array(acts))) * 100\n","\n","    print(f\"\\n {group} Evaluation:\")\n","    print(f\"  RMSE: {rmse:.4f}\")\n","    print(f\"  MAE : {mae:.4f}\")\n","    print(f\"  R²  : {r2:.4f}\")\n","    print(f\"  MAPE: {mape:.2f}%\")\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.plot(acts, label=f\"{group} Actual\")\n","    plt.plot(preds, label=f\"{group} Predicted\")\n","    plt.title(f\"{group} - 1-Day Ahead Rolling Forecast (2021)\")\n","    plt.xlabel(\"Trading Days\")\n","    plt.ylabel(\"Close Price\")\n","    plt.legend()\n","    plt.grid()\n","    plt.show()"],"metadata":{"id":"MYEwnR8iomlU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Graph Neural Networks"],"metadata":{"id":"U75-Zq4DsSBR"}},{"cell_type":"markdown","source":["The 80 securities need to be loaded for graph construction. This takes 1-5 minutes. The current setup predicts SPY in 2021."],"metadata":{"id":"5crjw8lmvHse"}},{"cell_type":"code","source":["!pip install torch-geometric ta"],"metadata":{"id":"8ySDIuMUr4g7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","from torch_geometric.data import Data\n","from functools import reduce\n","import joblib\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","# Load and preprocess function (only scaled close price)\n","def load_and_scale_close(filepath):\n","    df = pd.read_csv(filepath, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df = df[['Date', 'Close']]\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","\n","    scaler_close = MinMaxScaler()\n","    scaler = MinMaxScaler()\n","    df['Close'] = scaler_close.fit_transform(df[['Close']])\n","    df['RSI'] = scaler.fit_transform(df[['RSI']])\n","    df['MACD'] = scaler.fit_transform(df[['MACD']])\n","\n","    df = df[(df['Date'] >= '2015-01-01') & (df['Date'] <= '2021-12-31')]\n","    return df, scaler_close\n","\n","selected_stocks = ['AAPL', 'JPM', 'NVDA', 'GOOGL', 'MSFT', 'AMZN', 'META', #7\n","                   'TSLA', 'SONY', 'JBL', 'NFLX',  'AMD', 'TSM', 'ORCL', 'AVGO',\n","                   'V', 'MS', 'COF', 'MET', 'AMAT', 'MCHP', 'ADI', #22\n","                   'BAC', 'BLK', \"ASML\", 'QCOM', 'CRM', 'ADBE', 'GS', 'WFC', #30\n","                   'SCHW', 'BK', 'AXP', 'TXN', 'MU', 'NXPI', 'KLAC', 'LRCX', #38\n","                   'UNH', 'JNJ', 'PFE', 'MRK', 'LLY', 'TMO', 'BMY', 'LMT', 'F',\n","                   'GM', 'HMC', 'TM', 'WMT', 'HD', 'COST', 'PG', 'KO', 'MCD',#56\n","                   'TGT', 'PEP', 'JNJ', 'NVO'] #60\n","# Related to the 4 i need + healthcare, automotive and consumer, for diversity.\n","selected_etfs = ['SPY', 'QQQ', 'DIA', 'IWM', 'XLK', 'XLF', 'XLE', 'XLI', 'XLV',\n","                 'XLY', 'XLP', 'VNQ', 'IYR', 'VGT', 'VTI', 'VUG', 'VTV', 'IWF',\n","                 'IWD', 'ITOT'] # 20.\n","\n","target_stock = 'SPY'\n","\n","panel = {}\n","\n","# Load and scale AAPL separately to save its scaler\n","aapl_filepath = f\"{target_stock}.csv\"\n","aapl_df, aapl_scaler = load_and_scale_close(aapl_filepath)\n","panel[target_stock] = aapl_df\n","joblib.dump(aapl_scaler, f'scaler_{target_stock.lower()}.save')\n","\n","# Load and scale other tickers\n","tickers_to_process = [t for t in selected_stocks + selected_etfs if t != target_stock]\n","\n","for ticker in tickers_to_process:\n","    df, _ = load_and_scale_close(f\"{ticker}.csv\") # Scale each individually\n","    panel[ticker] = df\n","\n","\n","# Select only common dates\n","dataframes_to_merge = [df[['Date']] for df in panel.values()]\n","if not dataframes_to_merge:\n","    print(\"No dataframes loaded to find common dates.\")\n","else:\n","    common_dates = reduce(lambda x, y: pd.merge(x, y, on='Date', how='inner'), dataframes_to_merge)\n","    common_dates = pd.to_datetime(common_dates['Date'].unique())\n","    common_dates = sorted(common_dates.tolist()) # Corrected sorting\n","\n","\n","    # Filter dataframes by common dates\n","    for ticker in panel:\n","        panel[ticker] = panel[ticker][panel[ticker]['Date'].isin(common_dates)].reset_index(drop=True)\n","\n","    # Create graph list\n","    tickers_in_panel = list(panel.keys())\n","    target_indices = [tickers_in_panel.index(target_stock)]\n","\n","    def fully_connected_edges(num_nodes):\n","        row = torch.arange(num_nodes).repeat_interleave(num_nodes)\n","        col = torch.arange(num_nodes).repeat(num_nodes)\n","        return torch.stack([row, col], dim=0)\n","\n","    window_size = 21  # today + previous 20 days\n","\n","    graph_list = []\n","    num_days = len(common_dates) - 1  # Leave 1 for next-day label\n","\n","    for day in range(window_size - 1, num_days):\n","        x = []\n","        y = []\n","\n","        for ticker in tickers_to_process:\n","            # Collect features from day-(window_size-1) to day (inclusive)\n","            window_rows = panel[ticker].iloc[day - (window_size - 1): day + 1]\n","            # Extract features for each day in window: Close, RSI, MACD\n","            # Shape: (window_size, 3)\n","            features_window = window_rows[['Close', 'RSI', 'MACD']].values.flatten()\n","            x.append(features_window)\n","\n","        y.append(panel[target_stock].iloc[day + 1]['Close'])  # Next day close as label\n","\n","        x_tensor = torch.tensor(x, dtype=torch.float)\n","        y_tensor = torch.tensor(y, dtype=torch.float)\n","        x_tensor = torch.nan_to_num(x_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","        y_tensor = torch.nan_to_num(y_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        edge_index = fully_connected_edges(len(tickers_to_process))\n","        target_tensor = torch.tensor(target_indices, dtype=torch.long)\n","\n","        data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor, target_node_ids=target_tensor)\n","        graph_list.append(data)\n","\n","    torch.save(graph_list, 'graph_list_spy_21day_rsimacd21.pt')"],"metadata":{"id":"CHeceq4ps0px"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With the saved graph list, we can run the GAT to get predictions. This takes up to 5 minutes on the T4 GPU. This code returns the prediction and the attention weights of the 10 most related stocks."],"metadata":{"id":"IOk6Nj8lvSh-"}},{"cell_type":"code","source":["# === Model Definition ===\n","import torch\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# === Load Graph Data ===\n","target_stock = 'SPY'\n","graph_list = torch.load(f\"graph_list_{target_stock.lower()}_21day_rsimacd21.pt\", weights_only=False)\n","\n","test_days = 252\n","train_graphs = graph_list[:-test_days]\n","test_graphs = graph_list[-test_days:]\n","\n","batch_size = 1\n","train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n","\n","# === Model Definition ===\n","class GATPredictor(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout_prob=0.1):\n","        super(GATPredictor, self).__init__()\n","        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads)\n","        self.fc = nn.Linear(hidden_channels * heads, out_channels)  # Predict price for each node\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x, (edge_index_attn, attn_weights_tensor) = self.gat1(\n","            x, edge_index, return_attention_weights=True\n","        )\n","        x = F.elu(x)\n","        x = self.dropout(x)  # Apply dropout after activation\n","        output = self.fc(x)  # shape: [num_nodes, 1]\n","        return output, (edge_index_attn, attn_weights_tensor)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GATPredictor(in_channels=63, hidden_channels=32, out_channels=1).to(device) # Changed in_channels to 1\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","loss_fn = nn.MSELoss()\n","\n","# === Training / Testing ===\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out, _ = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","        out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","        # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","        if out_target_nodes.ndim == 0:\n","            out_target_nodes = out_target_nodes.unsqueeze(0)\n","        if data.y.ndim == 0:\n","            data.y = data.y.unsqueeze(0)\n","        loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","def test(loader):\n","    model.eval()\n","    total_loss = 0\n","    all_attn_weights = []\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            out, (edge_index_attn, attn_weights_tensor) = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","            out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","            # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","            if out_target_nodes.ndim == 0:\n","                out_target_nodes = out_target_nodes.unsqueeze(0)\n","            if data.y.ndim == 0:\n","                data.y = data.y.unsqueeze(0)\n","            loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","            total_loss += loss.item()\n","            all_attn_weights.append((edge_index_attn.cpu(), attn_weights_tensor.cpu())) # Store edge_index and attention_weights_tensor\n","\n","    return total_loss / len(loader), all_attn_weights\n","\n","import copy\n","\n","patience = 5   # how many epochs to wait for improvement\n","best_loss = float(\"inf\")\n","epochs_no_improve = 0\n","best_model_state = None\n","n_epochs = 30  # or however many max you want\n","\n","for epoch in range(n_epochs):\n","    train_loss = train()\n","    test_loss, test_attn_weights = test(test_loader)\n","\n","    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n","\n","    # --- Early stopping check (based on train loss) ---\n","    if train_loss < best_loss - 1e-6:   # small delta to avoid floating point noise\n","        best_loss = train_loss\n","        best_model_state = copy.deepcopy(model.state_dict())\n","        epochs_no_improve = 0\n","        print(f\" New best model saved (Train Loss: {best_loss:.4f})\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\" No improvement for {epochs_no_improve} epoch(s)\")\n","\n","    if epochs_no_improve >= patience:\n","        print(f\"\\n Early stopping at epoch {epoch+1}\")\n","        break\n","\n","# --- Restore best model before evaluation ---\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","    print(f\"\\n Restored model with best Train Loss = {best_loss:.4f}\")\n","\n","# === Predictions ===\n","model.eval()\n","predictions = []\n","actuals = []\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        data = data.to(device)\n","        out, _ = model(data)  # predictions for all nodes\n","        out_target = out[data.target_node_ids].squeeze().cpu().numpy()\n","        # Ensure out_target is treated as an iterable even if it's a scalar\n","        if out_target.ndim == 0:\n","            out_target = out_target.reshape(1)\n","        predictions.extend(out_target)\n","        actuals.extend(data.y.cpu().numpy())\n","\n","# === Inverse scaling and Metrics ===\n","aapl_scaler = joblib.load(f'scaler_{target_stock.lower()}.save')\n","\n","predictions = aapl_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n","actuals = aapl_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n","\n","rmse = np.sqrt(mean_squared_error(actuals, predictions))\n","mae = mean_absolute_error(actuals, predictions)\n","mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n","r2 = r2_score(actuals, predictions)\n","\n","print(f\"{target_stock} — RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n","\n","# === Plotting ===\n","plt.figure(figsize=(10, 4))\n","plt.plot(actuals, label=\"Actual\", color='black')\n","plt.plot(predictions, label=\"Predicted\", color='red', alpha=0.7)\n","plt.title(f\"{target_stock}: Actual vs Predicted Close Price (Over Time)\")\n","plt.xlabel(\"Days\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# ==================\n","# Attention Weights\n","# ==================\n","\n","# Extract attention weights for the target node\n","target_stock = 'SPY'\n","\n","tickers_in_panel = list(panel.keys())\n","aapl_node_index = tickers_in_panel.index(target_stock)\n","\n","aapl_attention_weights = []\n","\n","for edge_index, attn_weights in test_attn_weights:\n","    # Find edges where the target node is AAPL\n","    # In a fully connected graph, the target node is in the second row of edge_index\n","    # and its index is aapl_node_index\n","    aapl_edges_mask = edge_index[1] == aapl_node_index\n","\n","    # Get the attention weights for these edges\n","    weights_to_aapl = attn_weights[aapl_edges_mask]\n","    aapl_attention_weights.append(weights_to_aapl)\n","\n","# aapl_attention_weights is a list of tensors, where each tensor contains the attention\n","# weights from all other nodes towards AAPL for a given time step in the test set.\n","\n","print(f\"Extracted attention weights for {target_stock} for {len(aapl_attention_weights)} time steps.\")\n","\n","if aapl_attention_weights:\n","    print(f\"Shape of attention weights for one time step: {aapl_attention_weights[0].shape}\")\n","\n","\n","# Calculate average attention weights across the test set\n","# Assuming aapl_attention_weights is a list of tensors where each tensor is [num_nodes, heads]\n","# We want to average across the time steps (the list) and potentially across heads\n","\n","if aapl_attention_weights:\n","    # Concatenate all attention weight tensors from the list\n","    all_weights_tensor = torch.cat(aapl_attention_weights, dim=0) # Shape: [num_days * num_nodes, heads]\n","\n","    # Reshape to [num_days, num_nodes, heads] and average over days\n","    num_days_test = len(aapl_attention_weights)\n","    num_nodes = all_weights_tensor.shape[0] // num_days_test\n","    num_heads = all_weights_tensor.shape[1]\n","\n","    average_weights_per_node_head = all_weights_tensor.reshape(num_days_test, num_nodes, num_heads).mean(dim=0) # Shape: [num_nodes, heads]\n","\n","    # Average across heads to get a single weight per node\n","    average_weights_per_node = average_weights_per_node_head.mean(dim=1) # Shape: [num_nodes]\n","\n","    # Get the tickers corresponding to the nodes\n","    # Use tickers_to_process as the index, since the graph was built using these tickers\n","\n","    # Create a pandas Series for easier sorting and visualization\n","    average_attention_series = pd.Series(average_weights_per_node.numpy(), index=tickers_in_panel) # Use corrected index\n","\n","    # Sort the series by attention weight\n","    sorted_attention = average_attention_series.sort_values(ascending=False)\n","\n","    # Plot the top N attention weights\n","    top_n = 10 # Adjustable top_n\n","    plt.figure(figsize=(12, 6))\n","    sorted_attention.head(top_n).plot(kind='bar')\n","    plt.title(f\"Top {top_n} Average Attention Weights Towards {target_stock}\")\n","    plt.xlabel(\"Ticker\")\n","    plt.ylabel(\"Average Attention Weight\")\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Top {top_n} tickers by average attention weight towards {target_stock}:\")\n","    print(sorted_attention.head(top_n))\n","\n","else:\n","    print(\"No attention weights were extracted.\")"],"metadata":{"id":"mckuxLtluGy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. TFT GNN"],"metadata":{"id":"bk1YZ6B4xdsX"}},{"cell_type":"markdown","source":["To run all of the code necessary for the TFT-GNN to work can take up to 30 minutes on the T4 GPU. All 80 stocks must be loaded. GNN predictions are made for each of the target stocks: AAPL, JPM, NVDA, and SPY, and then fed into the TFT. The code is setup to predict 2024."],"metadata":{"id":"dFs8tVESxl0P"}},{"cell_type":"code","source":["!pip install pytorch_forecasting\n","!pip install torch\n","!pip install pytorch_lightning\n","!pip install torch-geometric ta"],"metadata":{"id":"wAtV6fJ3vfsm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Graph construction for AAPL, JPM, NVDA, and SPY."],"metadata":{"id":"yoq_KJ6pya8P"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","from torch_geometric.data import Data\n","from functools import reduce\n","import joblib\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","# Load and preprocess function (only scaled close price)\n","def load_and_scale_close(filepath):\n","    df = pd.read_csv(filepath, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df = df[['Date', 'Close']]\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","\n","    scaler_close = MinMaxScaler()\n","    scaler = MinMaxScaler()\n","    df['Close'] = scaler_close.fit_transform(df[['Close']])\n","    df['RSI'] = scaler.fit_transform(df[['RSI']])\n","    df['MACD'] = scaler.fit_transform(df[['MACD']])\n","\n","    # Find the date 21 entries before '2018-01-01' based on available dates\n","    cutoff_date = pd.to_datetime('2018-01-01')\n","    dates_before_cutoff = df[df['Date'] < cutoff_date]['Date'].sort_values(ascending=False).tolist()\n","    earliest_date_needed = dates_before_cutoff[min(len(dates_before_cutoff) - 1, 20)] if dates_before_cutoff else cutoff_date\n","\n","\n","    df = df[(df['Date'] >= earliest_date_needed) & (df['Date'] <= '2024-12-31')]\n","    return df, scaler_close\n","\n","selected_stocks = ['AAPL', 'JPM', 'NVDA', 'GOOGL', 'MSFT', 'AMZN', 'META', #7\n","                   'TSLA', 'SONY', 'JBL', 'NFLX',  'AMD', 'TSM', 'ORCL', 'AVGO',\n","                   'V', 'MS', 'COF', 'MET', 'AMAT', 'MCHP', 'ADI', #22\n","                   'BAC', 'BLK', \"ASML\", 'QCOM', 'CRM', 'ADBE', 'GS', 'WFC', #30\n","                   'SCHW', 'BK', 'AXP', 'TXN', 'MU', 'NXPI', 'KLAC', 'LRCX', #38\n","                   'UNH', 'JNJ', 'PFE', 'MRK', 'LLY', 'TMO', 'BMY', 'LMT', 'F',\n","                   'GM', 'HMC', 'TM', 'WMT', 'HD', 'COST', 'PG', 'KO', 'MCD',#56\n","                   'TGT', 'PEP', 'JNJ', 'NVO'] #60\n","# Related to the 4 i need + healthcare, automotive and consumer, for diversity.\n","selected_etfs = ['SPY', 'QQQ', 'DIA', 'IWM', 'XLK', 'XLF', 'XLE', 'XLI', 'XLV',\n","                 'XLY', 'XLP', 'VNQ', 'IYR', 'VGT', 'VTI', 'VUG', 'VTV', 'IWF',\n","                 'IWD', 'ITOT'] # 20.\n","\n","target_stock = 'AAPL'\n","\n","panel = {}\n","\n","# Load and scale AAPL separately to save its scaler\n","aapl_filepath = f\"{target_stock}.csv\"\n","aapl_df, aapl_scaler = load_and_scale_close(aapl_filepath)\n","panel[target_stock] = aapl_df\n","joblib.dump(aapl_scaler, f'scaler_{target_stock.lower()}.save')\n","\n","# Load and scale other tickers\n","tickers_to_process = [t for t in selected_stocks + selected_etfs if t != target_stock]\n","\n","for ticker in tickers_to_process:\n","    df, _ = load_and_scale_close(f\"{ticker}.csv\") # Scale each individually\n","    panel[ticker] = df\n","\n","\n","# Select only common dates\n","dataframes_to_merge = [df[['Date']] for df in panel.values()]\n","if not dataframes_to_merge:\n","    print(\"No dataframes loaded to find common dates.\")\n","else:\n","    common_dates = reduce(lambda x, y: pd.merge(x, y, on='Date', how='inner'), dataframes_to_merge)\n","    common_dates = pd.to_datetime(common_dates['Date'].unique())\n","    common_dates = sorted(common_dates.tolist()) # Corrected sorting\n","\n","\n","    # Filter dataframes by common dates\n","    for ticker in panel:\n","        panel[ticker] = panel[ticker][panel[ticker]['Date'].isin(common_dates)].reset_index(drop=True)\n","\n","    # Create graph list\n","    tickers_in_panel = list(panel.keys())\n","    target_indices = [tickers_in_panel.index(target_stock)]\n","\n","    def fully_connected_edges(num_nodes):\n","        row = torch.arange(num_nodes).repeat_interleave(num_nodes)\n","        col = torch.arange(num_nodes).repeat(num_nodes)\n","        return torch.stack([row, col], dim=0)\n","\n","    window_size = 21  # today + previous 20 days\n","\n","    graph_list = []\n","    num_days = len(common_dates) - 1  # Leave 1 for next-day label\n","\n","    for day in range(window_size - 1, num_days):\n","        x = []\n","        y = []\n","\n","        for ticker in tickers_to_process:\n","            # Collect features from day-(window_size-1) to day (inclusive)\n","            window_rows = panel[ticker].iloc[day - (window_size - 1): day + 1]\n","            # Extract features for each day in window: Close, RSI, MACD\n","            # Shape: (window_size, 3)\n","            features_window = window_rows[['Close']].values.flatten()\n","            x.append(features_window)\n","\n","        y.append(panel[target_stock].iloc[day + 1]['Close'])  # Next day close as label\n","\n","        x_tensor = torch.tensor(x, dtype=torch.float)\n","        y_tensor = torch.tensor(y, dtype=torch.float)\n","        x_tensor = torch.nan_to_num(x_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","        y_tensor = torch.nan_to_num(y_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        edge_index = fully_connected_edges(len(tickers_to_process))\n","        target_tensor = torch.tensor(target_indices, dtype=torch.long)\n","\n","        data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor, target_node_ids=target_tensor)\n","        graph_list.append(data)\n","\n","    torch.save(graph_list, 'graph_list_aapl_21day_rsimacd.pt')"],"metadata":{"id":"lMgY2CXByVBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","from torch_geometric.data import Data\n","from functools import reduce\n","import joblib\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","# Load and preprocess function (only scaled close price)\n","def load_and_scale_close(filepath):\n","    df = pd.read_csv(filepath, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df = df[['Date', 'Close']]\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","\n","    scaler_close = MinMaxScaler()\n","    scaler = MinMaxScaler()\n","    df['Close'] = scaler_close.fit_transform(df[['Close']])\n","    df['RSI'] = scaler.fit_transform(df[['RSI']])\n","    df['MACD'] = scaler.fit_transform(df[['MACD']])\n","\n","    # Find the date 21 entries before '2018-01-01' based on available dates\n","    cutoff_date = pd.to_datetime('2018-01-01')\n","    dates_before_cutoff = df[df['Date'] < cutoff_date]['Date'].sort_values(ascending=False).tolist()\n","    earliest_date_needed = dates_before_cutoff[min(len(dates_before_cutoff) - 1, 20)] if dates_before_cutoff else cutoff_date\n","\n","\n","    df = df[(df['Date'] >= earliest_date_needed) & (df['Date'] <= '2024-12-31')]\n","    return df, scaler_close\n","\n","selected_stocks = ['AAPL', 'JPM', 'NVDA', 'GOOGL', 'MSFT', 'AMZN', 'META', #7\n","                   'TSLA', 'SONY', 'JBL', 'NFLX',  'AMD', 'TSM', 'ORCL', 'AVGO',\n","                   'V', 'MS', 'COF', 'MET', 'AMAT', 'MCHP', 'ADI', #22\n","                   'BAC', 'BLK', \"ASML\", 'QCOM', 'CRM', 'ADBE', 'GS', 'WFC', #30\n","                   'SCHW', 'BK', 'AXP', 'TXN', 'MU', 'NXPI', 'KLAC', 'LRCX', #38\n","                   'UNH', 'JNJ', 'PFE', 'MRK', 'LLY', 'TMO', 'BMY', 'LMT', 'F',\n","                   'GM', 'HMC', 'TM', 'WMT', 'HD', 'COST', 'PG', 'KO', 'MCD',#56\n","                   'TGT', 'PEP', 'JNJ', 'NVO'] #60\n","# Related to the 4 i need + healthcare, automotive and consumer, for diversity.\n","selected_etfs = ['SPY', 'QQQ', 'DIA', 'IWM', 'XLK', 'XLF', 'XLE', 'XLI', 'XLV',\n","                 'XLY', 'XLP', 'VNQ', 'IYR', 'VGT', 'VTI', 'VUG', 'VTV', 'IWF',\n","                 'IWD', 'ITOT'] # 20.\n","\n","target_stock = 'JPM'\n","\n","panel = {}\n","\n","# Load and scale AAPL separately to save its scaler\n","aapl_filepath = f\"{target_stock}.csv\"\n","aapl_df, aapl_scaler = load_and_scale_close(aapl_filepath)\n","panel[target_stock] = aapl_df\n","joblib.dump(aapl_scaler, f'scaler_{target_stock.lower()}.save')\n","\n","# Load and scale other tickers\n","tickers_to_process = [t for t in selected_stocks + selected_etfs if t != target_stock]\n","\n","for ticker in tickers_to_process:\n","    df, _ = load_and_scale_close(f\"{ticker}.csv\") # Scale each individually\n","    panel[ticker] = df\n","\n","\n","# Select only common dates\n","dataframes_to_merge = [df[['Date']] for df in panel.values()]\n","if not dataframes_to_merge:\n","    print(\"No dataframes loaded to find common dates.\")\n","else:\n","    common_dates = reduce(lambda x, y: pd.merge(x, y, on='Date', how='inner'), dataframes_to_merge)\n","    common_dates = pd.to_datetime(common_dates['Date'].unique())\n","    common_dates = sorted(common_dates.tolist()) # Corrected sorting\n","\n","\n","    # Filter dataframes by common dates\n","    for ticker in panel:\n","        panel[ticker] = panel[ticker][panel[ticker]['Date'].isin(common_dates)].reset_index(drop=True)\n","\n","    # Create graph list\n","    tickers_in_panel = list(panel.keys())\n","    target_indices = [tickers_in_panel.index(target_stock)]\n","\n","    def fully_connected_edges(num_nodes):\n","        row = torch.arange(num_nodes).repeat_interleave(num_nodes)\n","        col = torch.arange(num_nodes).repeat(num_nodes)\n","        return torch.stack([row, col], dim=0)\n","\n","    window_size = 21  # today + previous 20 days\n","\n","    graph_list = []\n","    num_days = len(common_dates) - 1  # Leave 1 for next-day label\n","\n","    for day in range(window_size - 1, num_days):\n","        x = []\n","        y = []\n","\n","        for ticker in tickers_to_process:\n","            # Collect features from day-(window_size-1) to day (inclusive)\n","            window_rows = panel[ticker].iloc[day - (window_size - 1): day + 1]\n","            # Extract features for each day in window: Close, RSI, MACD\n","            # Shape: (window_size, 3)\n","            features_window = window_rows[['Close']].values.flatten()\n","            x.append(features_window)\n","\n","        y.append(panel[target_stock].iloc[day + 1]['Close'])  # Next day close as label\n","\n","        x_tensor = torch.tensor(x, dtype=torch.float)\n","        y_tensor = torch.tensor(y, dtype=torch.float)\n","        x_tensor = torch.nan_to_num(x_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","        y_tensor = torch.nan_to_num(y_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        edge_index = fully_connected_edges(len(tickers_to_process))\n","        target_tensor = torch.tensor(target_indices, dtype=torch.long)\n","\n","        data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor, target_node_ids=target_tensor)\n","        graph_list.append(data)\n","\n","    torch.save(graph_list, 'graph_list_jpm_21day_rsimacd.pt')"],"metadata":{"id":"uH-4C0OszhPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","from torch_geometric.data import Data\n","from functools import reduce\n","import joblib\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","# Load and preprocess function (only scaled close price)\n","def load_and_scale_close(filepath):\n","    df = pd.read_csv(filepath, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df = df[['Date', 'Close']]\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","\n","    scaler_close = MinMaxScaler()\n","    scaler = MinMaxScaler()\n","    df['Close'] = scaler_close.fit_transform(df[['Close']])\n","    df['RSI'] = scaler.fit_transform(df[['RSI']])\n","    df['MACD'] = scaler.fit_transform(df[['MACD']])\n","\n","    # Find the date 21 entries before '2018-01-01' based on available dates\n","    cutoff_date = pd.to_datetime('2018-01-01')\n","    dates_before_cutoff = df[df['Date'] < cutoff_date]['Date'].sort_values(ascending=False).tolist()\n","    earliest_date_needed = dates_before_cutoff[min(len(dates_before_cutoff) - 1, 20)] if dates_before_cutoff else cutoff_date\n","\n","\n","    df = df[(df['Date'] >= earliest_date_needed) & (df['Date'] <= '2024-12-31')]\n","    return df, scaler_close\n","\n","selected_stocks = ['AAPL', 'JPM', 'NVDA', 'GOOGL', 'MSFT', 'AMZN', 'META', #7\n","                   'TSLA', 'SONY', 'JBL', 'NFLX',  'AMD', 'TSM', 'ORCL', 'AVGO',\n","                   'V', 'MS', 'COF', 'MET', 'AMAT', 'MCHP', 'ADI', #22\n","                   'BAC', 'BLK', \"ASML\", 'QCOM', 'CRM', 'ADBE', 'GS', 'WFC', #30\n","                   'SCHW', 'BK', 'AXP', 'TXN', 'MU', 'NXPI', 'KLAC', 'LRCX', #38\n","                   'UNH', 'JNJ', 'PFE', 'MRK', 'LLY', 'TMO', 'BMY', 'LMT', 'F',\n","                   'GM', 'HMC', 'TM', 'WMT', 'HD', 'COST', 'PG', 'KO', 'MCD',#56\n","                   'TGT', 'PEP', 'JNJ', 'NVO'] #60\n","# Related to the 4 i need + healthcare, automotive and consumer, for diversity.\n","selected_etfs = ['SPY', 'QQQ', 'DIA', 'IWM', 'XLK', 'XLF', 'XLE', 'XLI', 'XLV',\n","                 'XLY', 'XLP', 'VNQ', 'IYR', 'VGT', 'VTI', 'VUG', 'VTV', 'IWF',\n","                 'IWD', 'ITOT'] # 20.\n","\n","target_stock = 'NVDA'\n","\n","panel = {}\n","\n","# Load and scale AAPL separately to save its scaler\n","aapl_filepath = f\"{target_stock}.csv\"\n","aapl_df, aapl_scaler = load_and_scale_close(aapl_filepath)\n","panel[target_stock] = aapl_df\n","joblib.dump(aapl_scaler, f'scaler_{target_stock.lower()}.save')\n","\n","# Load and scale other tickers\n","tickers_to_process = [t for t in selected_stocks + selected_etfs if t != target_stock]\n","\n","for ticker in tickers_to_process:\n","    df, _ = load_and_scale_close(f\"{ticker}.csv\") # Scale each individually\n","    panel[ticker] = df\n","\n","\n","# Select only common dates\n","dataframes_to_merge = [df[['Date']] for df in panel.values()]\n","if not dataframes_to_merge:\n","    print(\"No dataframes loaded to find common dates.\")\n","else:\n","    common_dates = reduce(lambda x, y: pd.merge(x, y, on='Date', how='inner'), dataframes_to_merge)\n","    common_dates = pd.to_datetime(common_dates['Date'].unique())\n","    common_dates = sorted(common_dates.tolist()) # Corrected sorting\n","\n","\n","    # Filter dataframes by common dates\n","    for ticker in panel:\n","        panel[ticker] = panel[ticker][panel[ticker]['Date'].isin(common_dates)].reset_index(drop=True)\n","\n","    # Create graph list\n","    tickers_in_panel = list(panel.keys())\n","    target_indices = [tickers_in_panel.index(target_stock)]\n","\n","    def fully_connected_edges(num_nodes):\n","        row = torch.arange(num_nodes).repeat_interleave(num_nodes)\n","        col = torch.arange(num_nodes).repeat(num_nodes)\n","        return torch.stack([row, col], dim=0)\n","\n","    window_size = 21  # today + previous 20 days\n","\n","    graph_list = []\n","    num_days = len(common_dates) - 1  # Leave 1 for next-day label\n","\n","    for day in range(window_size - 1, num_days):\n","        x = []\n","        y = []\n","\n","        for ticker in tickers_to_process:\n","            # Collect features from day-(window_size-1) to day (inclusive)\n","            window_rows = panel[ticker].iloc[day - (window_size - 1): day + 1]\n","            # Extract features for each day in window: Close, RSI, MACD\n","            # Shape: (window_size, 3)\n","            features_window = window_rows[['Close']].values.flatten()\n","            x.append(features_window)\n","\n","        y.append(panel[target_stock].iloc[day + 1]['Close'])  # Next day close as label\n","\n","        x_tensor = torch.tensor(x, dtype=torch.float)\n","        y_tensor = torch.tensor(y, dtype=torch.float)\n","        x_tensor = torch.nan_to_num(x_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","        y_tensor = torch.nan_to_num(y_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        edge_index = fully_connected_edges(len(tickers_to_process))\n","        target_tensor = torch.tensor(target_indices, dtype=torch.long)\n","\n","        data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor, target_node_ids=target_tensor)\n","        graph_list.append(data)\n","\n","    torch.save(graph_list, 'graph_list_nvda_21day_rsimacd.pt')"],"metadata":{"id":"Z2lsCxdbzkyr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import torch\n","from torch_geometric.data import Data\n","from functools import reduce\n","import joblib\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","# Load and preprocess function (only scaled close price)\n","def load_and_scale_close(filepath):\n","    df = pd.read_csv(filepath, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df = df[['Date', 'Close']]\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","\n","    scaler_close = MinMaxScaler()\n","    scaler = MinMaxScaler()\n","    df['Close'] = scaler_close.fit_transform(df[['Close']])\n","    df['RSI'] = scaler.fit_transform(df[['RSI']])\n","    df['MACD'] = scaler.fit_transform(df[['MACD']])\n","\n","    # Find the date 21 entries before '2018-01-01' based on available dates\n","    cutoff_date = pd.to_datetime('2018-01-01')\n","    dates_before_cutoff = df[df['Date'] < cutoff_date]['Date'].sort_values(ascending=False).tolist()\n","    earliest_date_needed = dates_before_cutoff[min(len(dates_before_cutoff) - 1, 20)] if dates_before_cutoff else cutoff_date\n","\n","    df = df[(df['Date'] >= earliest_date_needed) & (df['Date'] <= '2024-12-31')]\n","    return df, scaler_close\n","\n","selected_stocks = ['AAPL', 'JPM', 'NVDA', 'GOOGL', 'MSFT', 'AMZN', 'META', #7\n","                   'TSLA', 'SONY', 'JBL', 'NFLX',  'AMD', 'TSM', 'ORCL', 'AVGO',\n","                   'V', 'MS', 'COF', 'MET', 'AMAT', 'MCHP', 'ADI', #22\n","                   'BAC', 'BLK', \"ASML\", 'QCOM', 'CRM', 'ADBE', 'GS', 'WFC', #30\n","                   'SCHW', 'BK', 'AXP', 'TXN', 'MU', 'NXPI', 'KLAC', 'LRCX', #38\n","                   'UNH', 'JNJ', 'PFE', 'MRK', 'LLY', 'TMO', 'BMY', 'LMT', 'F',\n","                   'GM', 'HMC', 'TM', 'WMT', 'HD', 'COST', 'PG', 'KO', 'MCD',#56\n","                   'TGT', 'PEP', 'JNJ', 'NVO'] #60\n","# Related to the 4 i need + healthcare, automotive and consumer, for diversity.\n","selected_etfs = ['SPY', 'QQQ', 'DIA', 'IWM', 'XLK', 'XLF', 'XLE', 'XLI', 'XLV',\n","                 'XLY', 'XLP', 'VNQ', 'IYR', 'VGT', 'VTI', 'VUG', 'VTV', 'IWF',\n","                 'IWD', 'ITOT'] # 20.\n","\n","target_stock = 'SPY'\n","\n","panel = {}\n","\n","# Load and scale AAPL separately to save its scaler\n","aapl_filepath = f\"{target_stock}.csv\"\n","aapl_df, aapl_scaler = load_and_scale_close(aapl_filepath)\n","panel[target_stock] = aapl_df\n","joblib.dump(aapl_scaler, f'scaler_{target_stock.lower()}.save')\n","\n","# Load and scale other tickers\n","tickers_to_process = [t for t in selected_stocks + selected_etfs if t != target_stock]\n","\n","for ticker in tickers_to_process:\n","    df, _ = load_and_scale_close(f\"{ticker}.csv\") # Scale each individually\n","    panel[ticker] = df\n","\n","\n","# Select only common dates\n","dataframes_to_merge = [df[['Date']] for df in panel.values()]\n","if not dataframes_to_merge:\n","    print(\"No dataframes loaded to find common dates.\")\n","else:\n","    common_dates = reduce(lambda x, y: pd.merge(x, y, on='Date', how='inner'), dataframes_to_merge)\n","    common_dates = pd.to_datetime(common_dates['Date'].unique())\n","    common_dates = sorted(common_dates.tolist()) # Corrected sorting\n","\n","\n","    # Filter dataframes by common dates\n","    for ticker in panel:\n","        panel[ticker] = panel[ticker][panel[ticker]['Date'].isin(common_dates)].reset_index(drop=True)\n","\n","    # Create graph list\n","    tickers_in_panel = list(panel.keys())\n","    target_indices = [tickers_in_panel.index(target_stock)]\n","\n","    def fully_connected_edges(num_nodes):\n","        row = torch.arange(num_nodes).repeat_interleave(num_nodes)\n","        col = torch.arange(num_nodes).repeat(num_nodes)\n","        return torch.stack([row, col], dim=0)\n","\n","    window_size = 21  # today + previous 20 days\n","\n","    graph_list = []\n","    num_days = len(common_dates) - 1  # Leave 1 for next-day label\n","\n","    for day in range(window_size - 1, num_days):\n","        x = []\n","        y = []\n","\n","        for ticker in tickers_to_process:\n","            # Collect features from day-(window_size-1) to day (inclusive)\n","            window_rows = panel[ticker].iloc[day - (window_size - 1): day + 1]\n","            # Extract features for each day in window: Close, RSI, MACD\n","            # Shape: (window_size, 3)\n","            features_window = window_rows[['Close']].values.flatten()\n","            x.append(features_window)\n","\n","        y.append(panel[target_stock].iloc[day + 1]['Close'])  # Next day close as label\n","\n","        x_tensor = torch.tensor(x, dtype=torch.float)\n","        y_tensor = torch.tensor(y, dtype=torch.float)\n","        x_tensor = torch.nan_to_num(x_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","        y_tensor = torch.nan_to_num(y_tensor, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        edge_index = fully_connected_edges(len(tickers_to_process))\n","        target_tensor = torch.tensor(target_indices, dtype=torch.long)\n","\n","        data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor, target_node_ids=target_tensor)\n","        graph_list.append(data)\n","\n","    torch.save(graph_list, 'graph_list_spy_21day_rsimacd.pt')"],"metadata":{"id":"qUeTVnfqzp6Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now using these 4 graph lists, we make GNN predictions."],"metadata":{"id":"dpUE2MPszwgh"}},{"cell_type":"code","source":["# === Model Definition ===\n","import torch\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# === Load Graph Data ===\n","target_stock = 'AAPL'\n","graph_list = torch.load(f\"graph_list_{target_stock.lower()}_21day_rsimacd.pt\", weights_only=False)\n","\n","test_days = 252\n","train_graphs = graph_list[:-test_days]\n","test_graphs = graph_list\n","\n","batch_size = 1\n","train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n","\n","# === Model Definition ===\n","class GATPredictor(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout_prob=0.2):\n","        super(GATPredictor, self).__init__()\n","        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads)\n","        self.fc = nn.Linear(hidden_channels * heads, out_channels)  # Predict price for each node\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x, (edge_index_attn, attn_weights_tensor) = self.gat1(\n","            x, edge_index, return_attention_weights=True\n","        )\n","        x = F.elu(x)\n","        x = self.dropout(x)  # Apply dropout after activation\n","        output = self.fc(x)  # shape: [num_nodes, 1]\n","        return output, (edge_index_attn, attn_weights_tensor)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GATPredictor(in_channels=21, hidden_channels=32, out_channels=1).to(device) # Changed in_channels to 1\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n","loss_fn = nn.MSELoss()\n","\n","# === Training / Testing ===\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out, _ = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","        out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","        # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","        if out_target_nodes.ndim == 0:\n","            out_target_nodes = out_target_nodes.unsqueeze(0)\n","        if data.y.ndim == 0:\n","            data.y = data.y.unsqueeze(0)\n","        loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","def test(loader):\n","    model.eval()\n","    total_loss = 0\n","    all_attn_weights = []\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            out, (edge_index_attn, attn_weights_tensor) = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","            out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","            # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","            if out_target_nodes.ndim == 0:\n","                out_target_nodes = out_target_nodes.unsqueeze(0)\n","            if data.y.ndim == 0:\n","                data.y = data.y.unsqueeze(0)\n","            loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","            total_loss += loss.item()\n","            all_attn_weights.append((edge_index_attn.cpu(), attn_weights_tensor.cpu())) # Store edge_index and attention_weights_tensor\n","\n","    return total_loss / len(loader), all_attn_weights\n","\n","import copy\n","\n","patience = 5   # how many epochs to wait for improvement\n","best_loss = float(\"inf\")\n","epochs_no_improve = 0\n","best_model_state = None\n","n_epochs = 30  # or however many max you want\n","\n","for epoch in range(n_epochs):\n","    train_loss = train()\n","    test_loss, test_attn_weights = test(test_loader)\n","\n","    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n","\n","    # --- Early stopping check (based on train loss) ---\n","    if test_loss < best_loss - 1e-6:   # small delta to avoid floating point noise\n","        best_loss = test_loss\n","        best_model_state = copy.deepcopy(model.state_dict())\n","        epochs_no_improve = 0\n","        print(f\" New best model saved (Train Loss: {best_loss:.4f})\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\" No improvement for {epochs_no_improve} epoch(s)\")\n","\n","    if epochs_no_improve >= patience:\n","        print(f\"\\n Early stopping at epoch {epoch+1}\")\n","        break\n","\n","# --- Restore best model before evaluation ---\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","    print(f\"\\n Restored model with best Train Loss = {best_loss:.4f}\")\n","\n","# === Predictions ===\n","model.eval()\n","predictions = []\n","actuals = []\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        data = data.to(device)\n","        out, _ = model(data)  # predictions for all nodes\n","        out_target = out[data.target_node_ids].squeeze().cpu().numpy()\n","        # Ensure out_target is treated as an iterable even if it's a scalar\n","        if out_target.ndim == 0:\n","            out_target = out_target.reshape(1)\n","        predictions.extend(out_target)\n","        actuals.extend(data.y.cpu().numpy())\n","\n","# === Inverse scaling and Metrics ===\n","aapl_scaler = joblib.load(f'scaler_{target_stock.lower()}.save')\n","\n","predictions = aapl_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n","actuals = aapl_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n","\n","rmse = np.sqrt(mean_squared_error(actuals, predictions))\n","mae = mean_absolute_error(actuals, predictions)\n","mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n","r2 = r2_score(actuals, predictions)\n","\n","print(f\"{target_stock} — RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n","\n","# === Plotting ===\n","plt.figure(figsize=(10, 4))\n","plt.plot(actuals, label=\"Actual\", color='black')\n","plt.plot(predictions, label=\"Predicted\", color='red', alpha=0.7)\n","plt.title(f\"{target_stock}: Actual vs Predicted Close Price (Over Time)\")\n","plt.xlabel(\"Days\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","gnn_predictions = predictions\n","\n","# assume gnn_predictions is a numpy array (len = N)\n","gnn_predictions = np.array(gnn_predictions)\n","\n","# Compare today vs yesterday\n","gnn_updown = np.zeros_like(gnn_predictions, dtype=int)\n","\n","for i in range(1, len(gnn_predictions)):\n","    if gnn_predictions[i] > gnn_predictions[i-1]:\n","        gnn_updown[i] = 1  # up\n","    else:\n","        gnn_updown[i] = 0  # down\n","\n","# Force the first value to be \"up\" as you requested\n","gnn_updown[0] = 1\n","gnn_aapl_updown = gnn_updown\n","\n","\n","\n","# Extract attention weights for the target node (AAPL)\n","target_stock = 'AAPL'\n","# Assuming 'tickers' list and 'panel' dictionary are available from previous cells\n","tickers_in_panel = list(panel.keys())\n","aapl_node_index = tickers_in_panel.index(target_stock)\n","\n","aapl_attention_weights = []\n","\n","for edge_index, attn_weights in test_attn_weights:\n","    # Find edges where the target node is AAPL\n","    # In a fully connected graph, the target node is in the second row of edge_index\n","    # and its index is aapl_node_index\n","    aapl_edges_mask = edge_index[1] == aapl_node_index\n","\n","    # Get the attention weights for these edges\n","    weights_to_aapl = attn_weights[aapl_edges_mask]\n","    aapl_attention_weights.append(weights_to_aapl)\n","\n","# aapl_attention_weights is a list of tensors, where each tensor contains the attention\n","# weights from all other nodes towards AAPL for a given time step in the test set.\n","\n","print(f\"Extracted attention weights for {target_stock} for {len(aapl_attention_weights)} time steps.\")\n","# You can inspect the shape of the weights for a single time step\n","if aapl_attention_weights:\n","    print(f\"Shape of attention weights for one time step: {aapl_attention_weights[0].shape}\")\n","\n","\n","# Calculate average attention weights across the test set\n","# Assuming aapl_attention_weights is a list of tensors where each tensor is [num_nodes, heads]\n","# We want to average across the time steps (the list) and potentially across heads\n","\n","if aapl_attention_weights:\n","    # Concatenate all attention weight tensors from the list\n","    all_weights_tensor = torch.cat(aapl_attention_weights, dim=0) # Shape: [num_days * num_nodes, heads]\n","\n","    # Reshape to [num_days, num_nodes, heads] and average over days\n","    num_days_test = len(aapl_attention_weights)\n","    num_nodes = all_weights_tensor.shape[0] // num_days_test\n","    num_heads = all_weights_tensor.shape[1]\n","\n","    average_weights_per_node_head = all_weights_tensor.reshape(num_days_test, num_nodes, num_heads).mean(dim=0) # Shape: [num_nodes, heads]\n","\n","    # Average across heads to get a single weight per node\n","    average_weights_per_node = average_weights_per_node_head.mean(dim=1) # Shape: [num_nodes]\n","\n","    # Get the tickers corresponding to the nodes\n","    # Use tickers_to_process as the index, since the graph was built using these tickers\n","\n","    # Create a pandas Series for easier sorting and visualization\n","    average_attention_series = pd.Series(average_weights_per_node.numpy(), index=tickers_in_panel) # Use corrected index\n","\n","    # Sort the series by attention weight\n","    sorted_attention = average_attention_series.sort_values(ascending=False)\n","\n","    # Plot the top N attention weights\n","    top_n = 10 # Adjusted top_n\n","    plt.figure(figsize=(12, 6))\n","    sorted_attention.head(top_n).plot(kind='bar')\n","    plt.title(f\"Top {top_n} Average Attention Weights Towards {target_stock}\")\n","    plt.xlabel(\"Ticker\")\n","    plt.ylabel(\"Average Attention Weight\")\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Top {top_n} tickers by average attention weight towards {target_stock}:\")\n","    print(sorted_attention.head(top_n))\n","\n","else:\n","    print(\"No attention weights were extracted.\")"],"metadata":{"id":"wFUe3zYBqnIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Model Definition ===\n","import torch\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# === Load Graph Data ===\n","target_stock = 'JPM'\n","graph_list = torch.load(f\"graph_list_{target_stock.lower()}_21day_rsimacd.pt\", weights_only=False)\n","\n","test_days = 252\n","train_graphs = graph_list[:-test_days]\n","test_graphs = graph_list\n","\n","batch_size = 1\n","train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n","\n","# === Model Definition ===\n","class GATPredictor(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout_prob=0.2):\n","        super(GATPredictor, self).__init__()\n","        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads)\n","        self.fc = nn.Linear(hidden_channels * heads, out_channels)  # Predict price for each node\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x, (edge_index_attn, attn_weights_tensor) = self.gat1(\n","            x, edge_index, return_attention_weights=True\n","        )\n","        x = F.elu(x)\n","        x = self.dropout(x)  # Apply dropout after activation\n","        output = self.fc(x)  # shape: [num_nodes, 1]\n","        return output, (edge_index_attn, attn_weights_tensor)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GATPredictor(in_channels=21, hidden_channels=32, out_channels=1).to(device) # Changed in_channels to 1\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n","loss_fn = nn.MSELoss()\n","\n","# === Training / Testing ===\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out, _ = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","        out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","        # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","        if out_target_nodes.ndim == 0:\n","            out_target_nodes = out_target_nodes.unsqueeze(0)\n","        if data.y.ndim == 0:\n","            data.y = data.y.unsqueeze(0)\n","        loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","def test(loader):\n","    model.eval()\n","    total_loss = 0\n","    all_attn_weights = []\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            out, (edge_index_attn, attn_weights_tensor) = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","            out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","            # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","            if out_target_nodes.ndim == 0:\n","                out_target_nodes = out_target_nodes.unsqueeze(0)\n","            if data.y.ndim == 0:\n","                data.y = data.y.unsqueeze(0)\n","            loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","            total_loss += loss.item()\n","            all_attn_weights.append((edge_index_attn.cpu(), attn_weights_tensor.cpu())) # Store edge_index and attention_weights_tensor\n","\n","    return total_loss / len(loader), all_attn_weights\n","\n","import copy\n","\n","patience = 5   # how many epochs to wait for improvement\n","best_loss = float(\"inf\")\n","epochs_no_improve = 0\n","best_model_state = None\n","n_epochs = 30  # or however many max you want\n","\n","for epoch in range(n_epochs):\n","    train_loss = train()\n","    test_loss, test_attn_weights = test(test_loader)\n","\n","    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n","\n","    # --- Early stopping check (based on train loss) ---\n","    if test_loss < best_loss - 1e-6:   # small delta to avoid floating point noise\n","        best_loss = test_loss\n","        best_model_state = copy.deepcopy(model.state_dict())\n","        epochs_no_improve = 0\n","        print(f\" New best model saved (Train Loss: {best_loss:.4f})\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\" No improvement for {epochs_no_improve} epoch(s)\")\n","\n","    if epochs_no_improve >= patience:\n","        print(f\"\\n Early stopping at epoch {epoch+1}\")\n","        break\n","\n","# --- Restore best model before evaluation ---\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","    print(f\"\\n Restored model with best Train Loss = {best_loss:.4f}\")\n","\n","# === Predictions ===\n","model.eval()\n","predictions = []\n","actuals = []\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        data = data.to(device)\n","        out, _ = model(data)  # predictions for all nodes\n","        out_target = out[data.target_node_ids].squeeze().cpu().numpy()\n","        # Ensure out_target is treated as an iterable even if it's a scalar\n","        if out_target.ndim == 0:\n","            out_target = out_target.reshape(1)\n","        predictions.extend(out_target)\n","        actuals.extend(data.y.cpu().numpy())\n","\n","# === Inverse scaling and Metrics ===\n","aapl_scaler = joblib.load(f'scaler_{target_stock.lower()}.save')\n","\n","predictions = aapl_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n","actuals = aapl_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n","\n","rmse = np.sqrt(mean_squared_error(actuals, predictions))\n","mae = mean_absolute_error(actuals, predictions)\n","mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n","r2 = r2_score(actuals, predictions)\n","\n","print(f\"{target_stock} — RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n","\n","# === Plotting ===\n","plt.figure(figsize=(10, 4))\n","plt.plot(actuals, label=\"Actual\", color='black')\n","plt.plot(predictions, label=\"Predicted\", color='red', alpha=0.7)\n","plt.title(f\"{target_stock}: Actual vs Predicted Close Price (Over Time)\")\n","plt.xlabel(\"Days\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","gnn_predictions = predictions\n","\n","# assume gnn_predictions is a numpy array (len = N)\n","gnn_predictions = np.array(gnn_predictions)\n","\n","# Compare today vs yesterday\n","gnn_updown = np.zeros_like(gnn_predictions, dtype=int)\n","\n","for i in range(1, len(gnn_predictions)):\n","    if gnn_predictions[i] > gnn_predictions[i-1]:\n","        gnn_updown[i] = 1  # up\n","    else:\n","        gnn_updown[i] = 0  # down\n","\n","# Force the first value to be \"up\" as you requested\n","gnn_updown[0] = 1\n","gnn_jpm_updown = gnn_updown\n","\n","\n","\n","# Extract attention weights for the target node (AAPL)\n","target_stock = 'JPM'\n","# Assuming 'tickers' list and 'panel' dictionary are available from previous cells\n","tickers_in_panel = list(panel.keys())\n","aapl_node_index = tickers_in_panel.index(target_stock)\n","\n","aapl_attention_weights = []\n","\n","for edge_index, attn_weights in test_attn_weights:\n","    # Find edges where the target node is AAPL\n","    # In a fully connected graph, the target node is in the second row of edge_index\n","    # and its index is aapl_node_index\n","    aapl_edges_mask = edge_index[1] == aapl_node_index\n","\n","    # Get the attention weights for these edges\n","    weights_to_aapl = attn_weights[aapl_edges_mask]\n","    aapl_attention_weights.append(weights_to_aapl)\n","\n","# aapl_attention_weights is a list of tensors, where each tensor contains the attention\n","# weights from all other nodes towards AAPL for a given time step in the test set.\n","\n","print(f\"Extracted attention weights for {target_stock} for {len(aapl_attention_weights)} time steps.\")\n","# You can inspect the shape of the weights for a single time step\n","if aapl_attention_weights:\n","    print(f\"Shape of attention weights for one time step: {aapl_attention_weights[0].shape}\")\n","\n","\n","# Calculate average attention weights across the test set\n","# Assuming aapl_attention_weights is a list of tensors where each tensor is [num_nodes, heads]\n","# We want to average across the time steps (the list) and potentially across heads\n","\n","if aapl_attention_weights:\n","    # Concatenate all attention weight tensors from the list\n","    all_weights_tensor = torch.cat(aapl_attention_weights, dim=0) # Shape: [num_days * num_nodes, heads]\n","\n","    # Reshape to [num_days, num_nodes, heads] and average over days\n","    num_days_test = len(aapl_attention_weights)\n","    num_nodes = all_weights_tensor.shape[0] // num_days_test\n","    num_heads = all_weights_tensor.shape[1]\n","\n","    average_weights_per_node_head = all_weights_tensor.reshape(num_days_test, num_nodes, num_heads).mean(dim=0) # Shape: [num_nodes, heads]\n","\n","    # Average across heads to get a single weight per node\n","    average_weights_per_node = average_weights_per_node_head.mean(dim=1) # Shape: [num_nodes]\n","\n","    # Get the tickers corresponding to the nodes\n","    # Use tickers_to_process as the index, since the graph was built using these tickers\n","\n","    # Create a pandas Series for easier sorting and visualization\n","    average_attention_series = pd.Series(average_weights_per_node.numpy(), index=tickers_in_panel) # Use corrected index\n","\n","    # Sort the series by attention weight\n","    sorted_attention = average_attention_series.sort_values(ascending=False)\n","\n","    # Plot the top N attention weights\n","    top_n = 10 # Adjusted top_n\n","    plt.figure(figsize=(12, 6))\n","    sorted_attention.head(top_n).plot(kind='bar')\n","    plt.title(f\"Top {top_n} Average Attention Weights Towards {target_stock}\")\n","    plt.xlabel(\"Ticker\")\n","    plt.ylabel(\"Average Attention Weight\")\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Top {top_n} tickers by average attention weight towards {target_stock}:\")\n","    print(sorted_attention.head(top_n))\n","\n","else:\n","    print(\"No attention weights were extracted.\")"],"metadata":{"id":"gED6wE7zq7fE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Model Definition ===\n","import torch\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# === Load Graph Data ===\n","target_stock = 'NVDA'\n","graph_list = torch.load(f\"graph_list_{target_stock.lower()}_21day_rsimacd.pt\", weights_only=False)\n","\n","test_days = 252\n","train_graphs = graph_list[:-test_days]\n","test_graphs = graph_list\n","\n","batch_size = 1\n","train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n","\n","# === Model Definition ===\n","class GATPredictor(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout_prob=0.2):\n","        super(GATPredictor, self).__init__()\n","        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads)\n","        self.fc = nn.Linear(hidden_channels * heads, out_channels)  # Predict price for each node\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x, (edge_index_attn, attn_weights_tensor) = self.gat1(\n","            x, edge_index, return_attention_weights=True\n","        )\n","        x = F.elu(x)\n","        x = self.dropout(x)  # Apply dropout after activation\n","        output = self.fc(x)  # shape: [num_nodes, 1]\n","        return output, (edge_index_attn, attn_weights_tensor)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GATPredictor(in_channels=21, hidden_channels=32, out_channels=1).to(device) # Changed in_channels to 1\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n","loss_fn = nn.MSELoss()\n","\n","# === Training / Testing ===\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out, _ = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","        out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","        # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","        if out_target_nodes.ndim == 0:\n","            out_target_nodes = out_target_nodes.unsqueeze(0)\n","        if data.y.ndim == 0:\n","            data.y = data.y.unsqueeze(0)\n","        loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","def test(loader):\n","    model.eval()\n","    total_loss = 0\n","    all_attn_weights = []\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            out, (edge_index_attn, attn_weights_tensor) = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","            out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","            # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","            if out_target_nodes.ndim == 0:\n","                out_target_nodes = out_target_nodes.unsqueeze(0)\n","            if data.y.ndim == 0:\n","                data.y = data.y.unsqueeze(0)\n","            loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","            total_loss += loss.item()\n","            all_attn_weights.append((edge_index_attn.cpu(), attn_weights_tensor.cpu())) # Store edge_index and attention_weights_tensor\n","\n","    return total_loss / len(loader), all_attn_weights\n","\n","import copy\n","\n","patience = 5   # how many epochs to wait for improvement\n","best_loss = float(\"inf\")\n","epochs_no_improve = 0\n","best_model_state = None\n","n_epochs = 30  # or however many max you want\n","\n","for epoch in range(n_epochs):\n","    train_loss = train()\n","    test_loss, test_attn_weights = test(test_loader)\n","\n","    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n","\n","    # --- Early stopping check (based on train loss) ---\n","    if test_loss < best_loss - 1e-6:   # small delta to avoid floating point noise\n","        best_loss = test_loss\n","        best_model_state = copy.deepcopy(model.state_dict())\n","        epochs_no_improve = 0\n","        print(f\" New best model saved (Train Loss: {best_loss:.4f})\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\" No improvement for {epochs_no_improve} epoch(s)\")\n","\n","    if epochs_no_improve >= patience:\n","        print(f\"\\n Early stopping at epoch {epoch+1}\")\n","        break\n","\n","# --- Restore best model before evaluation ---\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","    print(f\"\\n Restored model with best Train Loss = {best_loss:.4f}\")\n","\n","# === Predictions ===\n","model.eval()\n","predictions = []\n","actuals = []\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        data = data.to(device)\n","        out, _ = model(data)  # predictions for all nodes\n","        out_target = out[data.target_node_ids].squeeze().cpu().numpy()\n","        # Ensure out_target is treated as an iterable even if it's a scalar\n","        if out_target.ndim == 0:\n","            out_target = out_target.reshape(1)\n","        predictions.extend(out_target)\n","        actuals.extend(data.y.cpu().numpy())\n","\n","# === Inverse scaling and Metrics ===\n","aapl_scaler = joblib.load(f'scaler_{target_stock.lower()}.save')\n","\n","predictions = aapl_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n","actuals = aapl_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n","\n","rmse = np.sqrt(mean_squared_error(actuals, predictions))\n","mae = mean_absolute_error(actuals, predictions)\n","mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n","r2 = r2_score(actuals, predictions)\n","\n","print(f\"{target_stock} — RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n","\n","# === Plotting ===\n","plt.figure(figsize=(10, 4))\n","plt.plot(actuals, label=\"Actual\", color='black')\n","plt.plot(predictions, label=\"Predicted\", color='red', alpha=0.7)\n","plt.title(f\"{target_stock}: Actual vs Predicted Close Price (Over Time)\")\n","plt.xlabel(\"Days\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","gnn_predictions = predictions\n","\n","# assume gnn_predictions is a numpy array (len = N)\n","gnn_predictions = np.array(gnn_predictions)\n","\n","# Compare today vs yesterday\n","gnn_updown = np.zeros_like(gnn_predictions, dtype=int)\n","\n","for i in range(1, len(gnn_predictions)):\n","    if gnn_predictions[i] > gnn_predictions[i-1]:\n","        gnn_updown[i] = 1  # up\n","    else:\n","        gnn_updown[i] = 0  # down\n","\n","# Force the first value to be \"up\" as you requested\n","gnn_updown[0] = 1\n","gnn_nvda_updown = gnn_updown\n","\n","\n","\n","# Extract attention weights for the target node (AAPL)\n","target_stock = 'NVDA'\n","# Assuming 'tickers' list and 'panel' dictionary are available from previous cells\n","tickers_in_panel = list(panel.keys())\n","aapl_node_index = tickers_in_panel.index(target_stock)\n","\n","aapl_attention_weights = []\n","\n","for edge_index, attn_weights in test_attn_weights:\n","    # Find edges where the target node is AAPL\n","    # In a fully connected graph, the target node is in the second row of edge_index\n","    # and its index is aapl_node_index\n","    aapl_edges_mask = edge_index[1] == aapl_node_index\n","\n","    # Get the attention weights for these edges\n","    weights_to_aapl = attn_weights[aapl_edges_mask]\n","    aapl_attention_weights.append(weights_to_aapl)\n","\n","# aapl_attention_weights is a list of tensors, where each tensor contains the attention\n","# weights from all other nodes towards AAPL for a given time step in the test set.\n","\n","print(f\"Extracted attention weights for {target_stock} for {len(aapl_attention_weights)} time steps.\")\n","# You can inspect the shape of the weights for a single time step\n","if aapl_attention_weights:\n","    print(f\"Shape of attention weights for one time step: {aapl_attention_weights[0].shape}\")\n","\n","\n","# Calculate average attention weights across the test set\n","# Assuming aapl_attention_weights is a list of tensors where each tensor is [num_nodes, heads]\n","# We want to average across the time steps (the list) and potentially across heads\n","\n","if aapl_attention_weights:\n","    # Concatenate all attention weight tensors from the list\n","    all_weights_tensor = torch.cat(aapl_attention_weights, dim=0) # Shape: [num_days * num_nodes, heads]\n","\n","    # Reshape to [num_days, num_nodes, heads] and average over days\n","    num_days_test = len(aapl_attention_weights)\n","    num_nodes = all_weights_tensor.shape[0] // num_days_test\n","    num_heads = all_weights_tensor.shape[1]\n","\n","    average_weights_per_node_head = all_weights_tensor.reshape(num_days_test, num_nodes, num_heads).mean(dim=0) # Shape: [num_nodes, heads]\n","\n","    # Average across heads to get a single weight per node\n","    average_weights_per_node = average_weights_per_node_head.mean(dim=1) # Shape: [num_nodes]\n","\n","    # Get the tickers corresponding to the nodes\n","    # Use tickers_to_process as the index, since the graph was built using these tickers\n","\n","    # Create a pandas Series for easier sorting and visualization\n","    average_attention_series = pd.Series(average_weights_per_node.numpy(), index=tickers_in_panel) # Use corrected index\n","\n","    # Sort the series by attention weight\n","    sorted_attention = average_attention_series.sort_values(ascending=False)\n","\n","    # Plot the top N attention weights\n","    top_n = 10 # Adjusted top_n\n","    plt.figure(figsize=(12, 6))\n","    sorted_attention.head(top_n).plot(kind='bar')\n","    plt.title(f\"Top {top_n} Average Attention Weights Towards {target_stock}\")\n","    plt.xlabel(\"Ticker\")\n","    plt.ylabel(\"Average Attention Weight\")\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Top {top_n} tickers by average attention weight towards {target_stock}:\")\n","    print(sorted_attention.head(top_n))\n","\n","else:\n","    print(\"No attention weights were extracted.\")"],"metadata":{"id":"-30j5klCsaRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Model Definition ===\n","import torch\n","from torch_geometric.data import Data\n","from torch_geometric.loader import DataLoader\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# === Load Graph Data ===\n","target_stock = 'SPY'\n","graph_list = torch.load(f\"graph_list_{target_stock.lower()}_21day_rsimacd.pt\", weights_only=False)\n","\n","test_days = 252\n","train_graphs = graph_list[:-test_days]\n","test_graphs = graph_list\n","\n","batch_size = 1\n","train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_graphs, batch_size=batch_size, shuffle=False)\n","\n","# === Model Definition ===\n","class GATPredictor(nn.Module):\n","    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout_prob=0.2):\n","        super(GATPredictor, self).__init__()\n","        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads)\n","        self.fc = nn.Linear(hidden_channels * heads, out_channels)  # Predict price for each node\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x, (edge_index_attn, attn_weights_tensor) = self.gat1(\n","            x, edge_index, return_attention_weights=True\n","        )\n","        x = F.elu(x)\n","        x = self.dropout(x)  # Apply dropout after activation\n","        output = self.fc(x)  # shape: [num_nodes, 1]\n","        return output, (edge_index_attn, attn_weights_tensor)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = GATPredictor(in_channels=21, hidden_channels=32, out_channels=1).to(device) # Changed in_channels to 1\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n","loss_fn = nn.MSELoss()\n","\n","# === Training / Testing ===\n","def train():\n","    model.train()\n","    total_loss = 0\n","    for data in train_loader:\n","        data = data.to(device)\n","        optimizer.zero_grad()\n","        out, _ = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","        out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","        # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","        if out_target_nodes.ndim == 0:\n","            out_target_nodes = out_target_nodes.unsqueeze(0)\n","        if data.y.ndim == 0:\n","            data.y = data.y.unsqueeze(0)\n","        loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)\n","\n","def test(loader):\n","    model.eval()\n","    total_loss = 0\n","    all_attn_weights = []\n","    with torch.no_grad():\n","        for data in loader:\n","            data = data.to(device)\n","            out, (edge_index_attn, attn_weights_tensor) = model(data)  # predictions for all nodes, shape [num_nodes, 1]\n","            out_target_nodes = out[data.target_node_ids].squeeze() # select and squeeze predictions for target nodes, shape [num_targets]\n","            # Ensure shapes are compatible for MSELoss, unsqueeze if scalar\n","            if out_target_nodes.ndim == 0:\n","                out_target_nodes = out_target_nodes.unsqueeze(0)\n","            if data.y.ndim == 0:\n","                data.y = data.y.unsqueeze(0)\n","            loss = loss_fn(out_target_nodes, data.y.to(device)) # compare with actual targets, shape [num_targets]\n","            total_loss += loss.item()\n","            all_attn_weights.append((edge_index_attn.cpu(), attn_weights_tensor.cpu())) # Store edge_index and attention_weights_tensor\n","\n","    return total_loss / len(loader), all_attn_weights\n","\n","import copy\n","\n","patience = 5   # how many epochs to wait for improvement\n","best_loss = float(\"inf\")\n","epochs_no_improve = 0\n","best_model_state = None\n","n_epochs = 30  # or however many max you want\n","\n","for epoch in range(n_epochs):\n","    train_loss = train()\n","    test_loss, test_attn_weights = test(test_loader)\n","\n","    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n","\n","    # --- Early stopping check (based on train loss) ---\n","    if test_loss < best_loss - 1e-6:   # small delta to avoid floating point noise\n","        best_loss = test_loss\n","        best_model_state = copy.deepcopy(model.state_dict())\n","        epochs_no_improve = 0\n","        print(f\" New best model saved (Train Loss: {best_loss:.4f})\")\n","    else:\n","        epochs_no_improve += 1\n","        print(f\" No improvement for {epochs_no_improve} epoch(s)\")\n","\n","    if epochs_no_improve >= patience:\n","        print(f\"\\n Early stopping at epoch {epoch+1}\")\n","        break\n","\n","# --- Restore best model before evaluation ---\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","    print(f\"\\n Restored model with best Train Loss = {best_loss:.4f}\")\n","\n","# === Predictions ===\n","model.eval()\n","predictions = []\n","actuals = []\n","\n","with torch.no_grad():\n","    for data in test_loader:\n","        data = data.to(device)\n","        out, _ = model(data)  # predictions for all nodes\n","        out_target = out[data.target_node_ids].squeeze().cpu().numpy()\n","        # Ensure out_target is treated as an iterable even if it's a scalar\n","        if out_target.ndim == 0:\n","            out_target = out_target.reshape(1)\n","        predictions.extend(out_target)\n","        actuals.extend(data.y.cpu().numpy())\n","\n","# === Inverse scaling and Metrics ===\n","aapl_scaler = joblib.load(f'scaler_{target_stock.lower()}.save')\n","\n","predictions = aapl_scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n","actuals = aapl_scaler.inverse_transform(np.array(actuals).reshape(-1, 1)).flatten()\n","\n","rmse = np.sqrt(mean_squared_error(actuals, predictions))\n","mae = mean_absolute_error(actuals, predictions)\n","mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n","r2 = r2_score(actuals, predictions)\n","\n","print(f\"{target_stock} — RMSE: {rmse:.4f} | MAE: {mae:.4f} | MAPE: {mape:.2f}% | R²: {r2:.4f}\")\n","\n","# === Plotting ===\n","plt.figure(figsize=(10, 4))\n","plt.plot(actuals, label=\"Actual\", color='black')\n","plt.plot(predictions, label=\"Predicted\", color='red', alpha=0.7)\n","plt.title(f\"{target_stock}: Actual vs Predicted Close Price (Over Time)\")\n","plt.xlabel(\"Days\")\n","plt.ylabel(\"Close Price\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","gnn_predictions = predictions\n","\n","# assume gnn_predictions is a numpy array (len = N)\n","gnn_predictions = np.array(gnn_predictions)\n","\n","# Compare today vs yesterday\n","gnn_updown = np.zeros_like(gnn_predictions, dtype=int)\n","\n","for i in range(1, len(gnn_predictions)):\n","    if gnn_predictions[i] > gnn_predictions[i-1]:\n","        gnn_updown[i] = 1  # up\n","    else:\n","        gnn_updown[i] = 0  # down\n","\n","# Force the first value to be \"up\" as you requested\n","gnn_updown[0] = 1\n","gnn_spy_updown = gnn_updown\n","\n","# Extract attention weights for the target node (AAPL)\n","target_stock = 'SPY'\n","# Assuming 'tickers' list and 'panel' dictionary are available from previous cells\n","tickers_in_panel = list(panel.keys())\n","aapl_node_index = tickers_in_panel.index(target_stock)\n","\n","aapl_attention_weights = []\n","\n","for edge_index, attn_weights in test_attn_weights:\n","    # Find edges where the target node is AAPL\n","    # In a fully connected graph, the target node is in the second row of edge_index\n","    # and its index is aapl_node_index\n","    aapl_edges_mask = edge_index[1] == aapl_node_index\n","\n","    # Get the attention weights for these edges\n","    weights_to_aapl = attn_weights[aapl_edges_mask]\n","    aapl_attention_weights.append(weights_to_aapl)\n","\n","# aapl_attention_weights is a list of tensors, where each tensor contains the attention\n","# weights from all other nodes towards AAPL for a given time step in the test set.\n","\n","print(f\"Extracted attention weights for {target_stock} for {len(aapl_attention_weights)} time steps.\")\n","# You can inspect the shape of the weights for a single time step\n","if aapl_attention_weights:\n","    print(f\"Shape of attention weights for one time step: {aapl_attention_weights[0].shape}\")\n","\n","\n","# Calculate average attention weights across the test set\n","# Assuming aapl_attention_weights is a list of tensors where each tensor is [num_nodes, heads]\n","# We want to average across the time steps (the list) and potentially across heads\n","\n","if aapl_attention_weights:\n","    # Concatenate all attention weight tensors from the list\n","    all_weights_tensor = torch.cat(aapl_attention_weights, dim=0) # Shape: [num_days * num_nodes, heads]\n","\n","    # Reshape to [num_days, num_nodes, heads] and average over days\n","    num_days_test = len(aapl_attention_weights)\n","    num_nodes = all_weights_tensor.shape[0] // num_days_test\n","    num_heads = all_weights_tensor.shape[1]\n","\n","    average_weights_per_node_head = all_weights_tensor.reshape(num_days_test, num_nodes, num_heads).mean(dim=0) # Shape: [num_nodes, heads]\n","\n","    # Average across heads to get a single weight per node\n","    average_weights_per_node = average_weights_per_node_head.mean(dim=1) # Shape: [num_nodes]\n","\n","    # Get the tickers corresponding to the nodes\n","    # Use tickers_to_process as the index, since the graph was built using these tickers\n","\n","    # Create a pandas Series for easier sorting and visualization\n","    average_attention_series = pd.Series(average_weights_per_node.numpy(), index=tickers_in_panel) # Use corrected index\n","\n","    # Sort the series by attention weight\n","    sorted_attention = average_attention_series.sort_values(ascending=False)\n","\n","    # Plot the top N attention weights\n","    top_n = 10 # Adjusted top_n\n","    plt.figure(figsize=(12, 6))\n","    sorted_attention.head(top_n).plot(kind='bar')\n","    plt.title(f\"Top {top_n} Average Attention Weights Towards {target_stock}\")\n","    plt.xlabel(\"Ticker\")\n","    plt.ylabel(\"Average Attention Weight\")\n","    plt.xticks(rotation=45, ha='right')\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Top {top_n} tickers by average attention weight towards {target_stock}:\")\n","    print(sorted_attention.head(top_n))\n","\n","else:\n","    print(\"No attention weights were extracted.\")"],"metadata":{"id":"H6G7nGtis5me"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Concatenate each prediction:"],"metadata":{"id":"m2sUYIQY4NV9"}},{"cell_type":"code","source":["gnn_predictions = np.concatenate((gnn_aapl_updown, gnn_jpm_updown, gnn_nvda_updown, gnn_spy_updown))"],"metadata":{"id":"5pfhXwOluNEs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Feed the concatenated predictions into a normal TFT. This code also produces the feature importance enncoder."],"metadata":{"id":"q2c6Lz6xulU1"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import torch\n","from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n","from pytorch_forecasting.metrics import SMAPE\n","from pytorch_lightning import Trainer, LightningModule\n","from pytorch_lightning.callbacks import EarlyStopping\n","from pytorch_lightning.loggers import TensorBoardLogger\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from collections import defaultdict\n","import warnings\n","import logging\n","from ta.momentum import RSIIndicator\n","from ta.trend import MACD\n","\n","warnings.filterwarnings(\"ignore\")\n","logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\n","\n","# === Load and preprocess data for all stocks ===\n","def load_stock(file_path, stock_name, stock_or_etf):\n","    df = pd.read_csv(file_path, skiprows=3, header=None)\n","    df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']\n","    df['Date'] = pd.to_datetime(df['Date'])\n","    df = df.sort_values('Date')\n","    df['group'] = stock_name\n","    df['stock_or_etf'] = stock_or_etf\n","    df['RSI'] = RSIIndicator(df['Close']).rsi()\n","    df['MACD'] = MACD(df['Close']).macd()\n","    df['upper_shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)\n","    df['lower_shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']\n","    return df\n","\n","aapl = load_stock(\"AAPL.csv\", \"AAPL\", \"stock\")\n","jpm  = load_stock(\"JPM.csv\", \"JPM\", \"stock\")\n","nvda = load_stock(\"NVDA.csv\", \"NVDA\", \"stock\")\n","spy  = load_stock(\"SPY.csv\",  \"SPY\", \"etf\")\n","\n","df = pd.concat([aapl, jpm, nvda, spy], ignore_index=True)\n","df = df[(df['Date'] > '2018-01-01') & (df['Date'] <= '2024-12-31')]\n","df = df.sort_values(['group', 'Date'])\n","df['time_idx'] = df.groupby('group').cumcount()\n","df['gnn_pred'] = gnn_predictions\n","\n","# Split train/val and test (based on calendar year)\n","train_val_df = df[df['Date'] < \"2024-01-01\"].copy()\n","test_df = df[df['Date'] >= \"2023-11-16\"].copy()\n","\n","# Recalculate time_idx in test_df based on prior max index\n","last_time_idx = train_val_df.groupby(\"group\")[\"time_idx\"].max().reset_index()\n","test_df = test_df.merge(last_time_idx, on=\"group\", suffixes=(\"\", \"_last\"))\n","test_df[\"time_idx\"] = test_df.groupby(\"group\").cumcount() + test_df[\"time_idx_last\"] + 1\n","test_df.drop(columns=[\"time_idx_last\"], inplace=True)\n","\n","# === Define dataset parameters ===\n","max_encoder_length = 30\n","max_prediction_length = 1\n","training_cutoff = train_val_df[\"time_idx\"].max() - 252  # leave last 252 days for validation\n","\n","# === Training dataset ===\n","tft_dataset = TimeSeriesDataSet(\n","    train_val_df[train_val_df.time_idx <= training_cutoff],\n","    time_idx=\"time_idx\",\n","    target=\"Close\",\n","    group_ids=[\"group\"],\n","    max_encoder_length=max_encoder_length,\n","    max_prediction_length=max_prediction_length,\n","    time_varying_known_reals=[\"time_idx\"],\n","    time_varying_unknown_reals=[\"Close\", \"RSI\", \"MACD\", 'upper_shadow', 'lower_shadow', 'gnn_pred'],\n","    static_categoricals=[\"group\", \"stock_or_etf\"],\n","    allow_missing_timesteps=True\n",")\n","\n","# === Validation dataset (pre-2024 only) ===\n","val_df = train_val_df[(train_val_df.time_idx > (training_cutoff - max_encoder_length))]\n","validation = TimeSeriesDataSet.from_dataset(tft_dataset, val_df, stop_randomization=True)\n","\n","train_loader = tft_dataset.to_dataloader(train=True, batch_size=32, num_workers=0)\n","val_loader = validation.to_dataloader(train=False, batch_size=32, num_workers=0)\n","\n","# === Define LightningModule wrapper ===\n","class TFTLightningModule(LightningModule):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        out = self.model(x)\n","        y_pred = out[0]\n","        loss = self.model.loss(y_pred, y)\n","        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=x['encoder_target'].shape[0])\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        out = self.model(x)\n","        y_pred = out[0]\n","        loss = self.model.loss(y_pred, y)\n","        self.log(\"val_loss\", loss, prog_bar=True, batch_size=x['encoder_target'].shape[0])\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.model.parameters(), lr=self.model.hparams.learning_rate)\n","\n","# === Train TFT model ===\n","tft = TemporalFusionTransformer.from_dataset(\n","    tft_dataset,\n","    learning_rate=0.003,\n","    hidden_size=32,\n","    attention_head_size=2,\n","    dropout=0.1,\n","    loss=SMAPE(),\n","    log_interval=10,\n","    reduce_on_plateau_patience=4,\n",").to(\"cuda\")\n","\n","module = TFTLightningModule(tft)\n","\n","trainer = Trainer(\n","    max_epochs=20,\n","    gradient_clip_val=0.1,\n","    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=8, mode=\"min\")],\n","    limit_train_batches=30,\n","    logger=TensorBoardLogger(\"lightning_logs\")\n",")\n","\n","trainer.fit(module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n","\n","\n","# === Rolling prediction on 2024 test data ===\n","preds_by_group = defaultdict(list)\n","actuals_by_group = defaultdict(list)\n","\n","for group in test_df[\"group\"].unique():\n","    df_group = test_df[test_df[\"group\"] == group].copy()\n","    df_group = df_group.reset_index(drop=True)\n","    df_group[\"time_idx\"] = np.arange(len(df_group))  # reindex cleanly for this stock\n","\n","    steps = len(df_group[\"time_idx\"].unique()) - max_encoder_length - max_prediction_length\n","\n","    for offset in range(steps):\n","        decoder_end_time = offset + max_encoder_length + max_prediction_length - 1\n","        data_slice = df_group[df_group.time_idx <= decoder_end_time].tail(max_encoder_length + max_prediction_length)\n","\n","        if data_slice['time_idx'].nunique() < (max_encoder_length + max_prediction_length):\n","            continue\n","\n","        try:\n","            predict_dataset = TimeSeriesDataSet.from_dataset(tft_dataset, data_slice, stop_randomization=True)\n","            predict_loader = predict_dataset.to_dataloader(train=False, batch_size=1, num_workers=0)\n","\n","            prediction = tft.predict(predict_loader).cpu().numpy().flatten()[0]\n","            actual = data_slice.iloc[-1][\"Close\"]\n","\n","            preds_by_group[group].append(prediction)\n","            actuals_by_group[group].append(actual)\n","        except Exception as e:\n","            print(f\"Skipping {group} at offset {offset} due to error: {e}\")\n","\n","\n","# === Evaluation & Plotting ===\n","for group in preds_by_group:\n","    preds = preds_by_group[group]\n","    acts = actuals_by_group[group]\n","\n","    if len(preds) == 0 or len(acts) == 0:\n","        print(f\"\\nNot enough data to evaluate {group}. Skipping...\")\n","        continue\n","\n","    rmse = np.sqrt(mean_squared_error(acts, preds))\n","    mae = mean_absolute_error(acts, preds)\n","    r2 = r2_score(acts, preds)\n","    mape = np.mean(np.abs((np.array(acts) - np.array(preds)) / np.array(acts))) * 100\n","\n","    print(f\"\\n {group} Evaluation:\")\n","    print(f\"  RMSE: {rmse:.4f}\")\n","    print(f\"  MAE : {mae:.4f}\")\n","    print(f\"  R²  : {r2:.4f}\")\n","    print(f\"  MAPE: {mape:.2f}%\")\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.plot(acts, label=f\"{group} Actual\")\n","    plt.plot(preds, label=f\"{group} Predicted\")\n","    plt.title(f\"{group} - 1-Day Ahead Rolling Forecast (2024)\")\n","    plt.xlabel(\"Trading Days\")\n","    plt.ylabel(\"Close Price\")\n","    plt.legend()\n","    plt.grid()\n","    plt.show()\n","\n","\n","# Select a representative sample from validation set\n","interpretation_dataset = TimeSeriesDataSet.from_dataset(\n","    tft_dataset,\n","    val_df.tail(200),  # adjust size if needed\n","    stop_randomization=True\n",")\n","\n","interpretation_loader = interpretation_dataset.to_dataloader(train=False, batch_size=64, num_workers=0)\n","\n","# Get raw predictions and attention weights\n","raw_output = tft.predict(interpretation_loader, mode=\"raw\", return_x=True, return_index=True)\n","\n","# `raw_output` is a dictionary-like object\n","raw_predictions = raw_output[0]\n","x = raw_output[1]\n","\n","# Interpret attention / variable importance\n","interpretation = tft.interpret_output(raw_predictions, reduction=\"mean\")\n","\n","# Plot\n","tft.plot_interpretation(interpretation)"],"metadata":{"id":"tES_kyoG0tyQ"},"execution_count":null,"outputs":[]}]}